{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "- documentacion y fuentes:\n",
    "- - https://github.com/codificandobits/Traductor_con_redes_Transformer\n",
    "- - https://github.com/CyberZHG/keras-transformer\n",
    "- -  https://arxiv.org/abs/1706.03762 ( google )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencias entre secuenciales y transformers\n",
    "\n",
    "**Secuenciales**:\n",
    " - memoria a corto plazo (incluso si son LSTM).\n",
    " - Procesado palabra por palabra\n",
    "\n",
    " - Memoria a largo plazo usando un mecanismo llamado **atención**\n",
    " - procesamiento en paralelo.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso:\n",
    "\n",
<<<<<<< HEAD
    "<img src = 'transformer/images/diagrama_transformers.png'>"
=======
    "<img src = 'images/diagrama_transformers.png'>"
>>>>>>> f592a47 (feature: investigación de avances en transformers)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding Entrada: Convierte el texto en una serie de vectores, en tokens (representación numerica).\n",
    "\n",
    "\n",
    "- Codificacion de posicion: La secuencia se procesa en paralelo, es necesario indicar el orden en que se encuentran las palabras en el texto. Este codificador de posiciones genera vectores que se sumaran a los tokens y que indican la posicion relativa de cada token dentro de la secuencia.\n",
    "\n",
    "(pueden existir varios :))\n",
    "- **Bloque de codificacion**.\n",
    "  - - **atencional**: se encarga de analizar la totalidad de la secuencia y buscar relaciones a diferentes niveles dentro de la secuencia.\n",
    "Los tokens se llevan a 3 redes neuronales entrenadas para encontrar 3 vectores. **Queries**, **keys** y **values**. (3 representaciones alternativas de los tokes originales).\n",
    "\n",
    "Luego cada queries se compara con los keys, con esto se obtiene un puntaje para el grado de asociacion de pares de palabras \n",
    "(por ejemplo con similitud de coseno.). Con estos puntajes se pondera cada uno de los vectores **values**\n",
    "<center>\n",
    "<img src = 'images/atencional.png'>\n",
    "</center>\n",
    "\n",
    "Es necesario escalar los puntajes, escalandolos dividiendolo entre el tamaño de cada vector y llevandolo a una función softmax.\n",
    "\n",
    "<center>\n",
    "<img src = './images/diagrama_transformers_2.png'>\n",
    "</center>\n",
    "\n",
    "Luego se multiplica esta matrix con la matriz de values. Estos serán (tantos como palabras hay) nuevos tokes con la codificación de la información de contexto más relevante para cada palabra de la secuencia.\n",
    "\n",
    "**nota:** Un bloque atencional no es suficiente. Ya que por ejemplo \"I Love Euclidean Geometry\" puede tener varias asociaciones en un bloque atencional.  [I Love] [Euclidean Geometry] o [I] [Love] [Euclidean Geometry]. Es decir además de asociaciones entre palabras hay asociaciones entre frases.\n",
    "\n",
    "Si se usan multiples bloques atencionales, es posible encontrar asociaciones de palabras y frases en diferente niveles. Luego si se tienen $k$ bloques atencionales, estos se combinan con una red neuronal en un único vector por cada token\n",
    "\n",
    "<center>\n",
    "<img src = './images/diagrama_transformers_3.png'>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "  - -  **bloque residual:** En este bloque se lleva tanto la entrada como la salida del bloque atencional. Luego suma y normaliza estos datos para tener una escala apropiada para el siguiente bloque.\n",
    "\n",
    "  <center>\n",
    "<img src = './images/diagrama_transformers_4.png'>\n",
    "</center>\n",
    "\n",
    "\n",
    "  - - **red neuronal + bloque residual:**\n",
    "  La red neuronal procesa en paralelo todos los vectores de la secuencia. tomanda la info atencional de las capas anteriores y consolidandolas en una única representación. La entrada y salida de esta red neuronal son enviadas a un bloque residual que tiene las mismas caracteristicas del bloque anterior\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = 'images/decodificador_1.png'>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Embedding Salida** y **Codificación posicional**. (analogo a los anteriores anterior)\n",
    "\n",
    "- **Bloque Atencional con enmascaramiento** Codifica la relación entre diferentes elementos de salida, usando los **Queries** , **Keys** y **Values**, la gran diferencia es que cada palabra se genera de manera secuencial, el decodificador solo debe prestar atención debe prestar atención **unicamente** alas palabras generas anteriormente y no a las futuras\n",
    "\n",
    "<center>\n",
    "<img src = 'images/decodificador_2.png'>\n",
    "</center>\n",
    "\n",
    "- **Bloque atencional (decodificador)** A diferencia del bloque atencional anterior (codificación) este bloque centra su atención en la secuencia original como la de salida. Toma la salida del codificador a las redes **Queries** y **Keys**. Mientras que la red **values** tiene como entrada el dato proveniente del bloque residual anterior.\n",
    "\n",
    "Asi el codificar le comunica al decodificador a que elementos debe prestar mas atencios al momemnto de generar la secuencia de salida (analogamente se toman muchos bloques atencionales para codificar asociaciones). Luego de pasar por los decodificadores\n",
    "genera un vector con cantidades numericas.\n",
    "\n",
    "**Capa Lineal** Toma el resultado del decodificador y lo transforma en un vector más grande. Por ejemplo si el traductor aprende 10.000 palabras, entonces el vector de salida de la capa lineal tendra 10.000 elementos\n",
    "\n",
    "**Softmax** Esta capa convierte cada elemento de este vector en sus respectivas probabilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo traductor (ingles a español)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras_transformer import get_model, decode\n",
    "from pickle import load\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/english-spanish.pkl'\n",
    "df = load(open(filename , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:500,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['go', 've'],\n",
       "       ['go', 'vete'],\n",
       "       ['go', 'vaya'],\n",
       "       ['go', 'vayase'],\n",
       "       ['hi', 'hola'],\n",
       "       ['run', 'corre'],\n",
       "       ['run', 'corran'],\n",
       "       ['run', 'corra'],\n",
       "       ['run', 'corred'],\n",
       "       ['run', 'corred']], dtype='<U275')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( f'palabra en ingles: {df[1221,0]}')\n",
    "# print( f'palabra en español: {df[1221,1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palabra en ingles: be good\n",
      "palabra en ingles tokenizada: ['be', 'good']\n",
      "\n",
      "palabra en español: sean buenos\n",
      "palabra en español tokenizada: ['sean', 'buenos']\n"
     ]
    }
   ],
   "source": [
    "# Creacion de \"tokens\"\n",
    "# Transforma cada palabra en una lista de palabras, esto por cada palabra.\n",
    "\n",
    "# tokens en ingles\n",
    "source_tokens = []\n",
    "for sentencia in df[:, 0]:\n",
    "    source_tokens.append(sentencia.split(' '))\n",
    "\n",
    "# Tokens en español\n",
    "target_tokens = []\n",
    "for sentencia in df[:, 1]:\n",
    "    target_tokens.append(sentencia.split(' '))\n",
    "\n",
    "print(f'palabra en ingles: {df[120,0]}')\n",
    "print(f'palabra en ingles tokenizada: {source_tokens[120]}\\n')\n",
    "print(f'palabra en español: {df[120,1]}')\n",
    "print(f'palabra en español tokenizada: {target_tokens[120]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_dict(token_list):\n",
    "    \"\"\"Resumen\n",
    "\n",
    "    Args:\n",
    "        token_list: Una lista que contiene lista de frases tokenizadas.\n",
    "\n",
    "    Returns:\n",
    "        dict: retorna un diccionario, donde el 0 es padding, 1 es start 2 end, y el resto de palabras (todas diferentes) tiene como como key un único numero (el tamaño del diccionario antes de ser agregada).\n",
    "    \"\"\"\n",
    "    \n",
    "    token_dict = {\n",
    "        '<PAD>': 0,\n",
    "        '<START>': 1,\n",
    "        '<END>': 2\n",
    "    }\n",
    "    for tokens in token_list:\n",
    "        for token in tokens:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = len(token_dict)\n",
    "    return token_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<START>': 1,\n",
       " '<END>': 2,\n",
       " 've': 3,\n",
       " 'vete': 4,\n",
       " 'vaya': 5,\n",
       " 'vayase': 6,\n",
       " 'hola': 7,\n",
       " 'corre': 8,\n",
       " 'corran': 9,\n",
       " 'corra': 10,\n",
       " 'corred': 11,\n",
       " 'quien': 12,\n",
       " 'orale': 13,\n",
       " 'fuego': 14,\n",
       " 'incendio': 15,\n",
       " 'disparad': 16,\n",
       " 'ayuda': 17,\n",
       " 'socorro': 18,\n",
       " 'auxilio': 19,\n",
       " 'salta': 20,\n",
       " 'salte': 21,\n",
       " 'parad': 22,\n",
       " 'para': 23,\n",
       " 'pare': 24,\n",
       " 'espera': 25,\n",
       " 'esperen': 26,\n",
       " 'continua': 27,\n",
       " 'continue': 28,\n",
       " 'date': 29,\n",
       " 'prisa': 30,\n",
       " 'daos': 31,\n",
       " 'dese': 32,\n",
       " 'me': 33,\n",
       " 'oculte': 34,\n",
       " 'escondi': 35,\n",
       " 'ocultaba': 36,\n",
       " 'escondia': 37,\n",
       " 'corri': 38,\n",
       " 'corria': 39,\n",
       " 'lo': 40,\n",
       " 'intento': 41,\n",
       " 'he': 42,\n",
       " 'ganado': 43,\n",
       " 'oh': 44,\n",
       " 'no': 45,\n",
       " 'tomatelo': 46,\n",
       " 'con': 47,\n",
       " 'soda': 48,\n",
       " 'disparen': 49,\n",
       " 'dispara': 50,\n",
       " 'dispare': 51,\n",
       " 'sonrie': 52,\n",
       " 'al': 53,\n",
       " 'ataque': 54,\n",
       " 'atacad': 55,\n",
       " 'ataquen': 56,\n",
       " 'ataca': 57,\n",
       " 'levanta': 58,\n",
       " 'ahora': 59,\n",
       " 'mismo': 60,\n",
       " 'id': 61,\n",
       " 'vayan': 62,\n",
       " 'ya': 63,\n",
       " 'tengo': 64,\n",
       " 'pillas': 65,\n",
       " 'entendiste': 66,\n",
       " 'el': 67,\n",
       " 'corrio': 68,\n",
       " 'metete': 69,\n",
       " 'adentro': 70,\n",
       " 'abrazame': 71,\n",
       " 'preocupo': 72,\n",
       " 'cai': 73,\n",
       " 'hui': 74,\n",
       " 'escape': 75,\n",
       " 'huia': 76,\n",
       " 'escapaba': 77,\n",
       " 'yo': 78,\n",
       " 'se': 79,\n",
       " 'sali': 80,\n",
       " 'menti': 81,\n",
       " 'perdi': 82,\n",
       " 'dimito': 83,\n",
       " 'renuncie': 84,\n",
       " 'dejo': 85,\n",
       " 'cante': 86,\n",
       " 'llore': 87,\n",
       " 'lloraba': 88,\n",
       " 'estoy': 89,\n",
       " 'trabajando': 90,\n",
       " 'diecinueve': 91,\n",
       " 'levantado': 92,\n",
       " 'escucha': 93,\n",
       " 'escuche': 94,\n",
       " 'escuchen': 95,\n",
       " 'puede': 96,\n",
       " 'ser': 97,\n",
       " 'de': 98,\n",
       " 'ninguna': 99,\n",
       " 'manera': 100,\n",
       " 'imposible': 101,\n",
       " 'ningun': 102,\n",
       " 'modo': 103,\n",
       " 'eso': 104,\n",
       " 'nada': 105,\n",
       " 'ni': 106,\n",
       " 'cagando': 107,\n",
       " 'mangos': 108,\n",
       " 'minga': 109,\n",
       " 'en': 110,\n",
       " 'pedo': 111,\n",
       " 'serio': 112,\n",
       " 'la': 113,\n",
       " 'verdad': 114,\n",
       " 'gracias': 115,\n",
       " 'pruebalo': 116,\n",
       " 'procuramos': 117,\n",
       " 'ganamos': 118,\n",
       " 'por': 119,\n",
       " 'que': 120,\n",
       " 'preguntale': 121,\n",
       " 'a': 122,\n",
       " 'tom': 123,\n",
       " 'mantente': 124,\n",
       " 'calma': 125,\n",
       " 'estate': 126,\n",
       " 'tranquilo': 127,\n",
       " 'justo': 128,\n",
       " 'bueno': 129,\n",
       " 'buena': 130,\n",
       " 'sed': 131,\n",
       " 'buenas': 132,\n",
       " 'buenos': 133,\n",
       " 'sea': 134,\n",
       " 'sean': 135,\n",
       " 'gentiles': 136,\n",
       " 'agradable': 137,\n",
       " 'pirate': 138,\n",
       " 'llamame': 139,\n",
       " 'llamadme': 140,\n",
       " 'llamanos': 141,\n",
       " 'entre': 142,\n",
       " 'pase': 143,\n",
       " 'entren': 144,\n",
       " 'andale': 145,\n",
       " 'sueltalo': 146,\n",
       " 'agarra': 147,\n",
       " 'bajate': 148,\n",
       " 'sal': 149,\n",
       " 'salid': 150,\n",
       " 'salgan': 151,\n",
       " 'aqui': 152,\n",
       " 'largate': 153,\n",
       " 'salga': 154,\n",
       " 'largo': 155,\n",
       " 'calle': 156,\n",
       " 'larguese': 157,\n",
       " 'casa': 158,\n",
       " 'despacio': 159,\n",
       " 'hasta': 160,\n",
       " 'luego': 161,\n",
       " 'vista': 162,\n",
       " 'chau': 163,\n",
       " 'un': 164,\n",
       " 'momento': 165,\n",
       " 'segundo': 166,\n",
       " 'fuertemente': 167,\n",
       " 'vino': 168,\n",
       " 'renuncio': 169,\n",
       " 'ayudame': 170,\n",
       " 'echame': 171,\n",
       " 'una': 172,\n",
       " 'mano': 173,\n",
       " 'ayudanos': 174,\n",
       " 'golpea': 175,\n",
       " 'sostenlo': 176,\n",
       " 'sostenedlo': 177,\n",
       " 'sostengalo': 178,\n",
       " 'sostenganlo': 179,\n",
       " 'sostenganla': 180,\n",
       " 'sostengala': 181,\n",
       " 'sostenla': 182,\n",
       " 'sostenedla': 183,\n",
       " 'resista': 184,\n",
       " 'resiste': 185,\n",
       " 'abraza': 186,\n",
       " 'acuerdo': 187,\n",
       " 'incline': 188,\n",
       " 'congele': 189,\n",
       " 'hele': 190,\n",
       " 'quede': 191,\n",
       " 'quieto': 192,\n",
       " 'mudado': 193,\n",
       " 'mude': 194,\n",
       " 'traslade': 195,\n",
       " 'trasladado': 196,\n",
       " 'dormi': 197,\n",
       " 'intente': 198,\n",
       " 'ire': 199,\n",
       " 'soy': 200,\n",
       " 'gordo': 201,\n",
       " 'gorda': 202,\n",
       " 'forma': 203,\n",
       " 'afectado': 204,\n",
       " 'viejo': 205,\n",
       " 'timido': 206,\n",
       " 'mojada': 207,\n",
       " 'esta': 208,\n",
       " 'bien': 209,\n",
       " 'unete': 210,\n",
       " 'nosotros': 211,\n",
       " 'parte': 212,\n",
       " 'nuestra': 213,\n",
       " 'guardalo': 214,\n",
       " 'pasa': 215,\n",
       " 'desapercibido': 216,\n",
       " 'tambien': 217,\n",
       " 'abre': 218,\n",
       " 'perfecto': 219,\n",
       " 'empujala': 220,\n",
       " 'empujalo': 221,\n",
       " 'empujadlo': 222,\n",
       " 'empujadla': 223,\n",
       " 'empujelo': 224,\n",
       " 'empujela': 225,\n",
       " 'empujenlo': 226,\n",
       " 'empujenla': 227,\n",
       " 'nos': 228,\n",
       " 'vemos': 229,\n",
       " 'muestramelo': 230,\n",
       " 'ensenamelo': 231,\n",
       " 'mostrame': 232,\n",
       " 'cierra': 233,\n",
       " 'boca': 234,\n",
       " 'cerra': 235,\n",
       " 'pico': 236,\n",
       " 'saltatelo': 237,\n",
       " 'paralo': 238,\n",
       " 'detenlo': 239,\n",
       " 'cogelo': 240,\n",
       " 'decime': 241,\n",
       " 'cuentame': 242,\n",
       " 'comio': 243,\n",
       " 'gano': 244,\n",
       " 'esperame': 245,\n",
       " 'despierto': 246,\n",
       " 'despierta': 247,\n",
       " 'despiertate': 248,\n",
       " 'despertate': 249,\n",
       " 'lavate': 250,\n",
       " 'las': 251,\n",
       " 'manos': 252,\n",
       " 'preocupamos': 253,\n",
       " 'sabemos': 254,\n",
       " 'perdimos': 255,\n",
       " 'bienvenidos': 256,\n",
       " 'bienvenidas': 257,\n",
       " 'ha': 258,\n",
       " 'has': 259,\n",
       " 'preguntales': 260,\n",
       " 'aparta': 261,\n",
       " 'retrocede': 262,\n",
       " 'apartate': 263,\n",
       " 'hombre': 264,\n",
       " 'fuerte': 265,\n",
       " 'breve': 266,\n",
       " 'breves': 267,\n",
       " 'te': 268,\n",
       " 'muevas': 269,\n",
       " 'llamalo': 270,\n",
       " 'tomas': 271,\n",
       " 'llamenlo': 272,\n",
       " 'animate': 273,\n",
       " 'venga': 274,\n",
       " 'calmate': 275,\n",
       " 'esposale': 276,\n",
       " 'vayas': 277,\n",
       " 'encontralo': 278,\n",
       " 'encuentralo': 279,\n",
       " 'encuentrelo': 280,\n",
       " 'encuentrenlo': 281,\n",
       " 'encuentre': 282,\n",
       " 'encuentren': 283,\n",
       " 'arregla': 284,\n",
       " 'esto': 285,\n",
       " 'ponte': 286,\n",
       " 'ello': 287,\n",
       " 'abajo': 288,\n",
       " 'tumbate': 289,\n",
       " 'pierdete': 290,\n",
       " 'esfumate': 291,\n",
       " 'piensalo': 292,\n",
       " 'los': 293,\n",
       " 'ojos': 294,\n",
       " 'adelante': 295,\n",
       " 'entra': 296,\n",
       " 'entrad': 297,\n",
       " 'subes': 298,\n",
       " 'buen': 299,\n",
       " 'trabajo': 300,\n",
       " 'sujeta': 301,\n",
       " 'agarralo': 302,\n",
       " 'diviertanse': 303,\n",
       " 'pasala': 304,\n",
       " 'pasenla': 305,\n",
       " 'hablo': 306,\n",
       " 'intenta': 307,\n",
       " 'prueba': 308,\n",
       " 'hay': 309,\n",
       " 'ey': 310,\n",
       " 'chavales': 311,\n",
       " 'troncos': 312,\n",
       " 'tal': 313,\n",
       " 'os': 314,\n",
       " 'va': 315,\n",
       " 'encanto': 316,\n",
       " 'tan': 317,\n",
       " 'profundo': 318,\n",
       " 'como': 319,\n",
       " 'hazme': 320,\n",
       " 'favor': 321,\n",
       " 'complaceme': 322,\n",
       " 'apresurate': 323,\n",
       " 'apurese': 324,\n",
       " 'convine': 325,\n",
       " 'acepte': 326,\n",
       " 'accedi': 327,\n",
       " 'comi': 328,\n",
       " 'puedo': 329,\n",
       " 'ir': 330,\n",
       " 'maldije': 331,\n",
       " 'despotrique': 332,\n",
       " 'hice': 333,\n",
       " 'fracase': 334,\n",
       " 'olvide': 335,\n",
       " 'arreglo': 336,\n",
       " 'entiendo': 337,\n",
       " 'llame': 338,\n",
       " 'telefono': 339,\n",
       " 'niego': 340,\n",
       " 'sente': 341,\n",
       " 'vi': 342,\n",
       " 'suspire': 343,\n",
       " 'sonrei': 344,\n",
       " 'hable': 345,\n",
       " 'hablaba': 346,\n",
       " 'charle': 347,\n",
       " 'charlaba': 348,\n",
       " 'uso': 349,\n",
       " 'espere': 350,\n",
       " 'pagare': 351,\n",
       " 'vuelto': 352,\n",
       " 'vuelta': 353,\n",
       " 'calvo': 354,\n",
       " 'calmado': 355,\n",
       " 'terminado': 356,\n",
       " 'acabado': 357,\n",
       " 'bastante': 358,\n",
       " 'facilon': 359,\n",
       " 'perfectamente': 360,\n",
       " 'libre': 361,\n",
       " 'lleno': 362,\n",
       " 'llena': 363,\n",
       " 'llene': 364,\n",
       " 'herido': 365,\n",
       " 'llego': 366,\n",
       " 'tarde': 367,\n",
       " 'vago': 368,\n",
       " 'perdida': 369,\n",
       " 'malo': 370,\n",
       " 'toca': 371,\n",
       " 'mi': 372,\n",
       " 'pobre': 373,\n",
       " 'rica': 374,\n",
       " 'rico': 375,\n",
       " 'salvo': 376,\n",
       " 'enferma': 377,\n",
       " 'delgado': 378,\n",
       " 'limpio': 379,\n",
       " 'fea': 380,\n",
       " 'calentito': 381,\n",
       " 'debil': 382,\n",
       " 'listo': 383,\n",
       " 'gane': 384,\n",
       " 'duele': 385,\n",
       " 'funciona': 386,\n",
       " 'es': 387,\n",
       " 'divertido': 388,\n",
       " 'suyo': 389,\n",
       " 'nuevo': 390,\n",
       " 'extrano': 391,\n",
       " 'rojo': 392,\n",
       " 'triste': 393,\n",
       " 'prohibido': 394,\n",
       " 'pasar': 395,\n",
       " 'entrar': 396,\n",
       " 'besa': 397,\n",
       " 'besen': 398,\n",
       " 'dejalo': 399,\n",
       " 'dejame': 400,\n",
       " 'dejanos': 401,\n",
       " 'vamonos': 402,\n",
       " 'vamos': 403,\n",
       " 'cuidado': 404,\n",
       " 'casate': 405,\n",
       " 'conmigo': 406,\n",
       " 'salva': 407,\n",
       " 'ella': 408,\n",
       " 'murio': 409,\n",
       " 'sentate': 410,\n",
       " 'sientate': 411,\n",
       " 'mas': 412,\n",
       " 'habla': 413,\n",
       " 'alto': 414,\n",
       " 'preparate': 415,\n",
       " 'parate': 416,\n",
       " 'pie': 417,\n",
       " 'deten': 418,\n",
       " 'llevate': 419,\n",
       " 'diselo': 420,\n",
       " 'genial': 421,\n",
       " 'ellos': 422,\n",
       " 'ganaron': 423,\n",
       " 'muerto': 424,\n",
       " 'cayo': 425,\n",
       " 'sabia': 426,\n",
       " 'tenia': 427,\n",
       " 'constancia': 428,\n",
       " 'fue': 429,\n",
       " 'mintio': 430,\n",
       " 'miente': 431,\n",
       " 'perdio': 432,\n",
       " 'pago': 433,\n",
       " 'nado': 434,\n",
       " 'lloro': 435,\n",
       " 'demasiado': 436,\n",
       " 'confia': 437,\n",
       " 'intentalo': 438,\n",
       " 'veras': 439,\n",
       " 'poco': 440,\n",
       " 'pruebe': 441,\n",
       " 'prueben': 442,\n",
       " 'proba': 443,\n",
       " 'usa': 444,\n",
       " 'avisale': 445,\n",
       " 'avisele': 446,\n",
       " 'avisenle': 447,\n",
       " 'mirame': 448,\n",
       " 'vigilame': 449,\n",
       " 'observame': 450,\n",
       " 'observalos': 451,\n",
       " 'observanos': 452,\n",
       " 'estamos': 453,\n",
       " 'intentamos': 454,\n",
       " 'iremos': 455}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokens_dict = build_token_dict(source_tokens)\n",
    "target_tokens_dict = build_token_dict(target_tokens)\n",
    "target_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diccionario con llave el label de la palabra y key la palabra\n",
    "target_tokens_dict_inv = {palabra: label for label,\n",
    "                          palabra in target_tokens_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<START>': 1,\n",
       " '<END>': 2,\n",
       " 'go': 3,\n",
       " 'hi': 4,\n",
       " 'run': 5,\n",
       " 'who': 6,\n",
       " 'wow': 7,\n",
       " 'fire': 8,\n",
       " 'help': 9,\n",
       " 'jump': 10,\n",
       " 'stop': 11,\n",
       " 'wait': 12,\n",
       " 'on': 13,\n",
       " 'hello': 14,\n",
       " 'hurry': 15,\n",
       " 'i': 16,\n",
       " 'hid': 17,\n",
       " 'ran': 18,\n",
       " 'try': 19,\n",
       " 'won': 20,\n",
       " 'oh': 21,\n",
       " 'no': 22,\n",
       " 'relax': 23,\n",
       " 'shoot': 24,\n",
       " 'smile': 25,\n",
       " 'attack': 26,\n",
       " 'get': 27,\n",
       " 'up': 28,\n",
       " 'now': 29,\n",
       " 'got': 30,\n",
       " 'it': 31,\n",
       " 'he': 32,\n",
       " 'hop': 33,\n",
       " 'in': 34,\n",
       " 'hug': 35,\n",
       " 'me': 36,\n",
       " 'care': 37,\n",
       " 'fell': 38,\n",
       " 'fled': 39,\n",
       " 'know': 40,\n",
       " 'left': 41,\n",
       " 'lied': 42,\n",
       " 'lost': 43,\n",
       " 'quit': 44,\n",
       " 'sang': 45,\n",
       " 'wept': 46,\n",
       " 'work': 47,\n",
       " 'im': 48,\n",
       " 'listen': 49,\n",
       " 'way': 50,\n",
       " 'really': 51,\n",
       " 'thanks': 52,\n",
       " 'we': 53,\n",
       " 'why': 54,\n",
       " 'ask': 55,\n",
       " 'tom': 56,\n",
       " 'awesome': 57,\n",
       " 'be': 58,\n",
       " 'calm': 59,\n",
       " 'cool': 60,\n",
       " 'fair': 61,\n",
       " 'good': 62,\n",
       " 'kind': 63,\n",
       " 'nice': 64,\n",
       " 'beat': 65,\n",
       " 'call': 66,\n",
       " 'us': 67,\n",
       " 'come': 68,\n",
       " 'drop': 69,\n",
       " 'out': 70,\n",
       " 'away': 71,\n",
       " 'home': 72,\n",
       " 'slow': 73,\n",
       " 'goodbye': 74,\n",
       " 'hang': 75,\n",
       " 'came': 76,\n",
       " 'hit': 77,\n",
       " 'hold': 78,\n",
       " 'agree': 79,\n",
       " 'bowed': 80,\n",
       " 'fired': 81,\n",
       " 'froze': 82,\n",
       " 'moved': 83,\n",
       " 'slept': 84,\n",
       " 'tried': 85,\n",
       " 'ill': 86,\n",
       " 'fat': 87,\n",
       " 'fit': 88,\n",
       " 'old': 89,\n",
       " 'shy': 90,\n",
       " 'wet': 91,\n",
       " 'its': 92,\n",
       " 'ok': 93,\n",
       " 'join': 94,\n",
       " 'keep': 95,\n",
       " 'lie': 96,\n",
       " 'low': 97,\n",
       " 'too': 98,\n",
       " 'open': 99,\n",
       " 'perfect': 100,\n",
       " 'push': 101,\n",
       " 'see': 102,\n",
       " 'you': 103,\n",
       " 'show': 104,\n",
       " 'shut': 105,\n",
       " 'skip': 106,\n",
       " 'so': 107,\n",
       " 'long': 108,\n",
       " 'take': 109,\n",
       " 'tell': 110,\n",
       " 'ate': 111,\n",
       " 'wake': 112,\n",
       " 'wash': 113,\n",
       " 'welcome': 114,\n",
       " 'not': 115,\n",
       " 'am': 116,\n",
       " 'them': 117,\n",
       " 'back': 118,\n",
       " 'off': 119,\n",
       " 'a': 120,\n",
       " 'man': 121,\n",
       " 'brave': 122,\n",
       " 'brief': 123,\n",
       " 'quiet': 124,\n",
       " 'still': 125,\n",
       " 'cheer': 126,\n",
       " 'cuff': 127,\n",
       " 'him': 128,\n",
       " 'dont': 129,\n",
       " 'drive': 130,\n",
       " 'find': 131,\n",
       " 'fix': 132,\n",
       " 'this': 133,\n",
       " 'down': 134,\n",
       " 'real': 135,\n",
       " 'ahead': 136,\n",
       " 'going': 137,\n",
       " 'job': 138,\n",
       " 'grab': 139,\n",
       " 'have': 140,\n",
       " 'fun': 141,\n",
       " 'spoke': 142,\n",
       " 'tries': 143,\n",
       " 'guys': 144,\n",
       " 'how': 145,\n",
       " 'cute': 146,\n",
       " 'deep': 147,\n",
       " 'humor': 148,\n",
       " 'agreed': 149,\n",
       " 'can': 150,\n",
       " 'cursed': 151,\n",
       " 'did': 152,\n",
       " 'failed': 153,\n",
       " 'forgot': 154,\n",
       " 'by': 155,\n",
       " 'jumped': 156,\n",
       " 'phoned': 157,\n",
       " 'refuse': 158,\n",
       " 'resign': 159,\n",
       " 'sat': 160,\n",
       " 'saw': 161,\n",
       " 'sighed': 162,\n",
       " 'smiled': 163,\n",
       " 'stayed': 164,\n",
       " 'talked': 165,\n",
       " 'use': 166,\n",
       " 'waited': 167,\n",
       " 'pay': 168,\n",
       " 'bald': 169,\n",
       " 'done': 170,\n",
       " 'easy': 171,\n",
       " 'fine': 172,\n",
       " 'free': 173,\n",
       " 'full': 174,\n",
       " 'here': 175,\n",
       " 'hurt': 176,\n",
       " 'late': 177,\n",
       " 'lazy': 178,\n",
       " 'mean': 179,\n",
       " 'next': 180,\n",
       " 'okay': 181,\n",
       " 'poor': 182,\n",
       " 'rich': 183,\n",
       " 'safe': 184,\n",
       " 'sick': 185,\n",
       " 'thin': 186,\n",
       " 'tidy': 187,\n",
       " 'ugly': 188,\n",
       " 'warm': 189,\n",
       " 'weak': 190,\n",
       " 'wise': 191,\n",
       " 'ive': 192,\n",
       " 'helps': 193,\n",
       " 'hurts': 194,\n",
       " 'works': 195,\n",
       " 'his': 196,\n",
       " 'new': 197,\n",
       " 'odd': 198,\n",
       " 'red': 199,\n",
       " 'sad': 200,\n",
       " 'kiss': 201,\n",
       " 'leave': 202,\n",
       " 'lets': 203,\n",
       " 'look': 204,\n",
       " 'marry': 205,\n",
       " 'may': 206,\n",
       " 'save': 207,\n",
       " 'she': 208,\n",
       " 'died': 209,\n",
       " 'runs': 210,\n",
       " 'sit': 211,\n",
       " 'speak': 212,\n",
       " 'stand': 213,\n",
       " 'stay': 214,\n",
       " 'put': 215,\n",
       " 'terrific': 216,\n",
       " 'they': 217,\n",
       " 'knew': 218,\n",
       " 'lies': 219,\n",
       " 'paid': 220,\n",
       " 'swam': 221,\n",
       " 'toms': 222,\n",
       " 'trust': 223,\n",
       " 'hard': 224,\n",
       " 'some': 225,\n",
       " 'warn': 226,\n",
       " 'watch': 227,\n",
       " 'well': 228,\n",
       " 'were': 229,\n",
       " 'what': 230,\n",
       " 'for': 231}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar start, end y pad a cada frase del entrenamiento\n",
    "\n",
    "encoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "decoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "\n",
    "# salida del transformer.\n",
    "output_tokens = [tokens + ['<END>'] for tokens in target_tokens]\n",
    "\n",
    "# Tamaño maximo de una palabra de entrada y de salida\n",
    "source_max_len = max(map(len, encoder_tokens))\n",
    "target_max_len = max(map(len, decoder_tokens))\n",
    "\n",
    "## Tenemos que entregar una lista con la misma dimensión. Dado que hay textos con diferentes tamaños \n",
    "encoder_tokens = [tokens + ['<PAD>'] *\n",
    "                  (source_max_len - len(tokens)) for tokens in encoder_tokens]\n",
    "\n",
    "decoder_tokens = [tokens + ['<PAD>'] *\n",
    "                  (target_max_len - len(tokens)) for tokens in decoder_tokens]\n",
    "\n",
    "\n",
    "## La salida final será en español por lo que se llenan espacios con\n",
    "output_tokens = [tokens + ['<PAD>'] *\n",
    "                  (target_max_len - len(tokens)) for tokens in output_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación posicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformar cada palabra en su respectiva etiqueta.\n",
    "encoder_input = [list(map(lambda x: source_tokens_dict[x], tokens))\n",
    "                 for tokens in encoder_tokens]\n",
    "\n",
    "decoder_input = [list(map(lambda x: target_tokens_dict[x], tokens))\n",
    "                 for tokens in decoder_tokens]\n",
    "\n",
    "## TODO revisar mas adelante minuto 29:41\n",
    "\n",
    "output_decoded = [list(map(lambda x: [target_tokens_dict[x]], tokens))  \n",
    "                  for tokens in output_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **token_num** : Tamaño máximo que puede llegar a tener cada frase.\n",
    "- **embed_dim** : Dimensión del embedding de entrada\n",
    "- **encoder_num**: Número de codificadores que tiene la entrada.\n",
    "- **decoder_num**: Número de decodificadores que tiene la salida\n",
    "- **head_num**: Número de bloques atencionales (para encontrar relaciones en diferentes niveles)\n",
    "- **hidden_dim**: Número de neuronas en la capa oculta (La capa verde en la teoria)\n",
    "- **dropout_rate**: Porcentajes de neuronas desactivadas aleatoriamente durante el proceso\n",
    "- **use_same_embed** (Boolean) donde \"False\" indica que la forma de representar la frase en ingles va a ser diferente a la forma en que va a encontrar las frases en español.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<START>': 1,\n",
       " '<END>': 2,\n",
       " 'go': 3,\n",
       " 'hi': 4,\n",
       " 'run': 5,\n",
       " 'who': 6,\n",
       " 'wow': 7,\n",
       " 'fire': 8,\n",
       " 'help': 9,\n",
       " 'jump': 10,\n",
       " 'stop': 11,\n",
       " 'wait': 12,\n",
       " 'on': 13,\n",
       " 'hello': 14,\n",
       " 'hurry': 15,\n",
       " 'i': 16,\n",
       " 'hid': 17,\n",
       " 'ran': 18,\n",
       " 'try': 19,\n",
       " 'won': 20,\n",
       " 'oh': 21,\n",
       " 'no': 22,\n",
       " 'relax': 23,\n",
       " 'shoot': 24,\n",
       " 'smile': 25,\n",
       " 'attack': 26,\n",
       " 'get': 27,\n",
       " 'up': 28,\n",
       " 'now': 29,\n",
       " 'got': 30,\n",
       " 'it': 31,\n",
       " 'he': 32,\n",
       " 'hop': 33,\n",
       " 'in': 34,\n",
       " 'hug': 35,\n",
       " 'me': 36,\n",
       " 'care': 37,\n",
       " 'fell': 38,\n",
       " 'fled': 39,\n",
       " 'know': 40,\n",
       " 'left': 41,\n",
       " 'lied': 42,\n",
       " 'lost': 43,\n",
       " 'quit': 44,\n",
       " 'sang': 45,\n",
       " 'wept': 46,\n",
       " 'work': 47,\n",
       " 'im': 48,\n",
       " 'listen': 49,\n",
       " 'way': 50,\n",
       " 'really': 51,\n",
       " 'thanks': 52,\n",
       " 'we': 53,\n",
       " 'why': 54,\n",
       " 'ask': 55,\n",
       " 'tom': 56,\n",
       " 'awesome': 57,\n",
       " 'be': 58,\n",
       " 'calm': 59,\n",
       " 'cool': 60,\n",
       " 'fair': 61,\n",
       " 'good': 62,\n",
       " 'kind': 63,\n",
       " 'nice': 64,\n",
       " 'beat': 65,\n",
       " 'call': 66,\n",
       " 'us': 67,\n",
       " 'come': 68,\n",
       " 'drop': 69,\n",
       " 'out': 70,\n",
       " 'away': 71,\n",
       " 'home': 72,\n",
       " 'slow': 73,\n",
       " 'goodbye': 74,\n",
       " 'hang': 75,\n",
       " 'came': 76,\n",
       " 'hit': 77,\n",
       " 'hold': 78,\n",
       " 'agree': 79,\n",
       " 'bowed': 80,\n",
       " 'fired': 81,\n",
       " 'froze': 82,\n",
       " 'moved': 83,\n",
       " 'slept': 84,\n",
       " 'tried': 85,\n",
       " 'ill': 86,\n",
       " 'fat': 87,\n",
       " 'fit': 88,\n",
       " 'old': 89,\n",
       " 'shy': 90,\n",
       " 'wet': 91,\n",
       " 'its': 92,\n",
       " 'ok': 93,\n",
       " 'join': 94,\n",
       " 'keep': 95,\n",
       " 'lie': 96,\n",
       " 'low': 97,\n",
       " 'too': 98,\n",
       " 'open': 99,\n",
       " 'perfect': 100,\n",
       " 'push': 101,\n",
       " 'see': 102,\n",
       " 'you': 103,\n",
       " 'show': 104,\n",
       " 'shut': 105,\n",
       " 'skip': 106,\n",
       " 'so': 107,\n",
       " 'long': 108,\n",
       " 'take': 109,\n",
       " 'tell': 110,\n",
       " 'ate': 111,\n",
       " 'wake': 112,\n",
       " 'wash': 113,\n",
       " 'welcome': 114,\n",
       " 'not': 115,\n",
       " 'am': 116,\n",
       " 'them': 117,\n",
       " 'back': 118,\n",
       " 'off': 119,\n",
       " 'a': 120,\n",
       " 'man': 121,\n",
       " 'brave': 122,\n",
       " 'brief': 123,\n",
       " 'quiet': 124,\n",
       " 'still': 125,\n",
       " 'cheer': 126,\n",
       " 'cuff': 127,\n",
       " 'him': 128,\n",
       " 'dont': 129,\n",
       " 'drive': 130,\n",
       " 'find': 131,\n",
       " 'fix': 132,\n",
       " 'this': 133,\n",
       " 'down': 134,\n",
       " 'real': 135,\n",
       " 'ahead': 136,\n",
       " 'going': 137,\n",
       " 'job': 138,\n",
       " 'grab': 139,\n",
       " 'have': 140,\n",
       " 'fun': 141,\n",
       " 'spoke': 142,\n",
       " 'tries': 143,\n",
       " 'guys': 144,\n",
       " 'how': 145,\n",
       " 'cute': 146,\n",
       " 'deep': 147,\n",
       " 'humor': 148,\n",
       " 'agreed': 149,\n",
       " 'can': 150,\n",
       " 'cursed': 151,\n",
       " 'did': 152,\n",
       " 'failed': 153,\n",
       " 'forgot': 154,\n",
       " 'by': 155,\n",
       " 'jumped': 156,\n",
       " 'phoned': 157,\n",
       " 'refuse': 158,\n",
       " 'resign': 159,\n",
       " 'sat': 160,\n",
       " 'saw': 161,\n",
       " 'sighed': 162,\n",
       " 'smiled': 163,\n",
       " 'stayed': 164,\n",
       " 'talked': 165,\n",
       " 'use': 166,\n",
       " 'waited': 167,\n",
       " 'pay': 168,\n",
       " 'bald': 169,\n",
       " 'done': 170,\n",
       " 'easy': 171,\n",
       " 'fine': 172,\n",
       " 'free': 173,\n",
       " 'full': 174,\n",
       " 'here': 175,\n",
       " 'hurt': 176,\n",
       " 'late': 177,\n",
       " 'lazy': 178,\n",
       " 'mean': 179,\n",
       " 'next': 180,\n",
       " 'okay': 181,\n",
       " 'poor': 182,\n",
       " 'rich': 183,\n",
       " 'safe': 184,\n",
       " 'sick': 185,\n",
       " 'thin': 186,\n",
       " 'tidy': 187,\n",
       " 'ugly': 188,\n",
       " 'warm': 189,\n",
       " 'weak': 190,\n",
       " 'wise': 191,\n",
       " 'ive': 192,\n",
       " 'helps': 193,\n",
       " 'hurts': 194,\n",
       " 'works': 195,\n",
       " 'his': 196,\n",
       " 'new': 197,\n",
       " 'odd': 198,\n",
       " 'red': 199,\n",
       " 'sad': 200,\n",
       " 'kiss': 201,\n",
       " 'leave': 202,\n",
       " 'lets': 203,\n",
       " 'look': 204,\n",
       " 'marry': 205,\n",
       " 'may': 206,\n",
       " 'save': 207,\n",
       " 'she': 208,\n",
       " 'died': 209,\n",
       " 'runs': 210,\n",
       " 'sit': 211,\n",
       " 'speak': 212,\n",
       " 'stand': 213,\n",
       " 'stay': 214,\n",
       " 'put': 215,\n",
       " 'terrific': 216,\n",
       " 'they': 217,\n",
       " 'knew': 218,\n",
       " 'lies': 219,\n",
       " 'paid': 220,\n",
       " 'swam': 221,\n",
       " 'toms': 222,\n",
       " 'trust': 223,\n",
       " 'hard': 224,\n",
       " 'some': 225,\n",
       " 'warn': 226,\n",
       " 'watch': 227,\n",
       " 'well': 228,\n",
       " 'were': 229,\n",
       " 'what': 230,\n",
       " 'for': 231}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random([1,32]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "model = get_model(\n",
    "    token_num=max(len(source_tokens_dict), len(target_tokens_dict)),\n",
    "    embed_dim=32,\n",
    "    encoder_num=3,\n",
    "    decoder_num=2,\n",
    "    head_num=4,\n",
    "    hidden_dim=128,\n",
    "    dropout_rate=0.05,\n",
    "    use_same_embed=False,\n",
    ")\n",
    "\n",
    "# model.compile('adam','sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entrenamiento\n",
    "x = [np.array(encoder_input), np.array(decoder_input)]\n",
    "y = [np.array(output_decoded)]\n",
    "\n",
    "# No tengo suficiente memoria :( \n",
    "# model.fit(x,y, epochs=15, batch_size= 32)\n",
    "\n",
    "filename_model = 'models/translator.h5'\n",
    "model.load_weights(filename_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentencia):\n",
    "    sentencia_tokens = [tokens + ['<END>', '<PAD>']\n",
    "                        for tokens in [sentencia.split(' ')]]\n",
    "    tr_input = [list(map(lambda x: source_tokens_dict[x], tokens))\n",
    "                for tokens in sentencia_tokens][0]\n",
    "\n",
    "    # decodificacion\n",
    "    decoded = decode(\n",
    "        model,\n",
    "        tr_input,\n",
    "        start_token=target_tokens_dict['<START>'],\n",
    "        end_token=target_tokens_dict['<END>'],\n",
    "        pad_token=target_tokens_dict['<PAD>']\n",
    "    )\n",
    "    print(f\"Frase original {sentencia}\")\n",
    "    \n",
    "    print(f\"Frase traducida {' '.join(map(lambda x :target_tokens_dict_inv[x], decoded[1:-1]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tr_input**: Es la entrada del transformer, agregando < END>  y < PAD> (START no es necesario)\n",
    "\n",
    "- **decoded[1:-1]** me va a retornar los label de cada frase traducida, luego obtenemos la palabra correspondiente en target_tokens_dict_inv. \n",
    "\n",
    "- **start_token , end_token, pad_token**: deben ser los label que tomamos para indicar donde empieza una frase,donde termina y donde dejamos espacios respectivamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"The red car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers en imagenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documentacion : \n",
    "- https://arxiv.org/abs/2010.11929v2\n",
    "- https://pytorch.org/vision/main/models/vision_transformer.html\n",
    "- https://www.youtube.com/watch?v=j3VNqtJUoz0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los transformers fueron propuestos por Vaswani et al. (2017) para traducción automática.\n",
    "\n",
    "**Estado del arte:**\n",
    "-  La capa atencional en una imagen requeriría que cada píxel atienda a los demás\n",
    "píxel. Con un costo cuadrático en la cantidad de píxeles, esto no se escala a tamaños de entrada realistas. \n",
    "\n",
    "Parmar et al. (2018) aplicó self-attention en vecindades locales para cada consulta de pixel. Estos bloques locales \"multi-head dot-product self attention blocks\" pueden remplazar completamente las convolucionales\n",
    "\n",
    "-(Ho et al., 2019; Wang et al., 2020a). Muchas de estas arquitecturas de atención especializada demuestran\n",
    "resultados prometedores en tareas de visión por computadora, pero requieren una ingeniería compleja para implementarse de manera eficiente en aceleradores de hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/transformer_imagen_1.png\">\n",
    "</center>\n",
    "\n",
    "\n",
    "(**nota para desglosar**)\n",
    "\n",
    "- Se divide la imagen en $\\mathbb{R^{(28x28)}}$ en segmentos fijos, creando un array.de tamaño $\\mathbb{R^{784}}$. luego se agrega un position embeddings, luego se alimenta la secuencia de vectores a un Transformer encoder. Para realizar la clasificación, utilizamos el enfoque estándar de agregar un elemento adicional que se puede aprender (< START> < END>).\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/transformer_imagen.png\">\n",
    "</center>\n",
    "\n",
    "\n",
    "- ¿Que es Patching? Es una transformación de la imagen en una secuencia. Cada uno de los valores de color se colocan en neuronas y se pasan a través de capas lineales, la salida es un embedding. (En la figura se tiene que paarchamos (Patching) la imagen en 16 bloques, ahora a cada uno de estos parches se pasa sus colores a una red neuronal por cada color presente, esto es un embedding que se le suministra al transformer )\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/transformer_imagen_2.png\">\n",
    "</center>\n",
    "\n",
    "\n",
    "**VISION TRANSFORMER (VIT)**\n",
    "\n",
    "- El standard Transformer recibe como entrada un 1D secuencia de token embeddings. Para manejar imágenes 2D, remodelamos la imagen \n",
    "$x\\in\\mathbb{R}^{H x\n",
    " W x C}$ dentro de una secuencia de flattened 2d aplanados. $x_p \\in \\mathbb{R}^{N x (P^2\\cdot C)}$. donde $(H,W)$ es la resolución de la imagen original, $C$ es el número de canales, $(P,P)$ es la resolución de cada imagen aplanada, y  $N= \\dfrac{H\\cdot W}{P ^2 }$ es el npumero parches, que tambien sirve como una entrada efeciva para el trasformer.\n",
    "\n",
    "- **Nota:** Las imagenes tiene un ancho y alto especifico, ademas de tener 3 canales de colores. Dado que las imagenes pueden tener tamaños diferentes, entonces  ¿cómo se reorganiza la imagen en la parte superior en parches de un tamaño de parche especifico? **Einops:(inverstigar)** permite reorganizar matrices y tensores multidimensionales de varias maneras ( terminar..)\n",
    "\n",
    "\n",
    "**The CLS Token**\n",
    "Dado que el transformer es un modelo secuencia a secuencia, tendrá varias salidas ¿Cual es la mejor manera de obtener una representación global única?\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/transformer_imagen_2.png\">\n",
    "</center>\n",
    "\n",
    "Por cada uno de los parches que se tiene un véctor de, pero se tiene una entrada adiccional ( el cero ), este es el token de clase. Se puede ver como una entrada ficticia que luego se completa con información de todas las demas entradas. Inicialmente se llena con valores aleatorios y luego estos vectores se ajustan para que sea un vector  que se puede aprender y luego nos servirá  para extraer caracteristicas globales. A todas las imagenes de entrada se agrega un embedding de posición\n",
    "\n",
    "**The Positional Embedding**\n",
    "Al igual que el transformer tradicional, el transformer de vision (vision transformer). Una idea es agregarle un número a cada uno de los parches, pero esto no es eficiente ya que estos números pueden volverse muy grandes y no hay una relación natural con la imagen, lo que hace que el aprendizaje no sea tan bueno.\n",
    "\n",
    "Las incorporaciones posicionales presentadas en el articulo. En lugar de números son vectores y la distancia de entradas es consistente para diferentes longitudes, estos vectores se contruyen como una combinacion de senos y cosenos. En vision transformer el tamaño de los parches son fijos, por lo que no tenemos este problema... (**Desgrosar mas...**)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "    <img src=\"images/transformer_imagen_4.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Swin transformer?**\n",
    "\n",
    "Swin Transformer: https://arxiv.org/abs/2103.14030"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Arquitecturas hibridas con transformes?\n",
    "- TransUnet\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/transformer_imagen_5.png\">\n",
    "</center>\n",
    "... continuará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ConvMixers\n",
    "<center>\n",
    "    <img src=\"images/transformer_imagen_6.png\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Nuestro problema?\n",
    "- https://www.youtube.com/watch?v=S8ol2_s6Qr8&t=1967s   \n",
    "minuto 31:24\n",
    "\n",
    " - - https://github.com/mrcabellom/edge-video-services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
