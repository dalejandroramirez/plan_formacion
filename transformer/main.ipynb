{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "- documentacion y fuentes:\n",
    "- - https://github.com/codificandobits/Traductor_con_redes_Transformer\n",
    "- - https://github.com/CyberZHG/keras-transformer\n",
    "- -  https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencias entre secuenciales y transformers\n",
    "\n",
    "**Secuenciales**:\n",
    " - memoria a corto plazo (incluso si son LSTM).\n",
    " - Procesado palabra por palabra\n",
    "\n",
    " - Memoria a largo plazo usando un mecanismo llamado **atención**\n",
    " - procesamiento en paralelo.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso:\n",
    "\n",
    "<img src = './images/diagrama_transformers.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding Entrada: Convierte el texto en una serie de vectores, en tokens (representación numerica).\n",
    "\n",
    "\n",
    "- Codificacion de posicion: La secuencia se procesa en paralelo, es necesario indicar el orden en que se encuentran las palabras en el texto. Este codificador de posiciones genera vectores que se sumaran a los tokens y que indican la posicion relativa de cada token dentro de la secuencia.\n",
    "\n",
    "(pueden existir varios :))\n",
    "- **Bloque de codificacion**.\n",
    "  - - **atencional**: se encarga de analizar la totalidad de la secuencia y buscar relaciones a diferentes niveles dentro de la secuencia.\n",
    "Los tokens se llevan a 3 redes neuronales entrenadas para encontrar 3 vectores. **Queries**, **keys** y **values**. (3 representaciones alternativas de los tokes originales).\n",
    "\n",
    "Luego cada queries se compara con los keys, con esto se obtiene un puntaje para el grado de asociacion de pares de palabras \n",
    "(por ejemplo con similitud de coseno.). Con estos puntajes se pondera cada uno de los vectores **values**\n",
    "<center>\n",
    "<img src = 'images/atencional.png'>\n",
    "</center>\n",
    "\n",
    "Es necesario escalar los puntajes, escalandolos dividiendolo entre el tamaño de cada vector y llevandolo a una función softmax.\n",
    "\n",
    "<center>\n",
    "<img src = './images/diagrama_transformers_2.png'>\n",
    "</center>\n",
    "\n",
    "Luego se multiplica esta matrix con la matriz de values. Estos serán (tantos como palabras hay) nuevos tokes con la codificación de la información de contexto más relevante para cada palabra de la secuencia.\n",
    "\n",
    "**nota:** Un bloque atencional no es suficiente. Ya que por ejemplo \"I Love Euclidean Geometry\" puede tener varias asociaciones en un bloque atencional.  [I Love] [Euclidean Geometry] o [I] [Love] [Euclidean Geometry]. Es decir además de asociaciones entre palabras hay asociaciones entre frases.\n",
    "\n",
    "Si se usan multiples bloques atencionales, es posible encontrar asociaciones de palabras y frases en diferente niveles. Luego si se tienen $k$ bloques atencionales, estos se combinan con una red neuronal en un único vector por cada token\n",
    "\n",
    "<center>\n",
    "<img src = './images/diagrama_transformers_3.png'>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "  - -  **bloque residual:** En este bloque se lleva tanto la entrada como la salida del bloque atencional. Luego suma y normaliza estos datos para tener una escala apropiada para el siguiente bloque.\n",
    "\n",
    "  <center>\n",
    "<img src = './images/diagrama_transformers_4.png'>\n",
    "</center>\n",
    "\n",
    "\n",
    "  - - **red neuronal + bloque residual:**\n",
    "  La red neuronal procesa en paralelo todos los vectores de la secuencia. tomanda la info atencional de las capas anteriores y consolidandolas en una única representación. La entrada y salida de esta red neuronal son enviadas a un bloque residual que tiene las mismas caracteristicas del bloque anterior\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = 'images/decodificador_1.png'>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Embedding Salida** y **Codificación posicional**. (analogo a los anteriores anterior)\n",
    "\n",
    "- **Bloque Atencional con enmascaramiento** Codifica la relación entre diferentes elementos de salida, usando los **Queries** , **Keys** y **Values**, la gran diferencia es que cada palabra se genera de manera secuencial, el decodificador solo debe prestar atención debe prestar atención **unicamente** alas palabras generas anteriormente y no a las futuras\n",
    "\n",
    "<center>\n",
    "<img src = 'images/decodificador_2.png'>\n",
    "</center>\n",
    "\n",
    "- **Bloque atencional (decodificador)** A diferencia del bloque atencional anterior (codificación) este bloque centra su atención en la secuencia original como la de salida. Toma la salida del codificador a las redes **Queries** y **Keys**. Mientras que la red **values** tiene como entrada el dato proveniente del bloque residual anterior.\n",
    "\n",
    "Asi el codificar le comunica al decodificador a que elementos debe prestar mas atencios al momemnto de generar la secuencia de salida (analogamente se toman muchos bloques atencionales para codificar asociaciones). Luego de pasar por los decodificadores\n",
    "genera un vector con cantidades numericas.\n",
    "\n",
    "**Capa Lineal** Toma el resultado del decodificador y lo transforma en un vector más grande. Por ejemplo si el traductor aprende 10.000 palabras, entonces el vector de salida de la capa lineal tendra 10.000 elementos\n",
    "\n",
    "**Softmax** Esta capa convierte cada elemento de este vector en sus respectivas probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras_transformer import get_model, decode \n",
    "from pickle import load  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
