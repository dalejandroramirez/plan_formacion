{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "- documentacion y fuentes:\n",
    "- - https://github.com/codificandobits/Traductor_con_redes_Transformer\n",
    "- - https://github.com/CyberZHG/keras-transformer\n",
    "- -  https://arxiv.org/abs/1706.03762 ( google )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencias entre secuenciales y transformers\n",
    "\n",
    "**Secuenciales**:\n",
    " - memoria a corto plazo (incluso si son LSTM).\n",
    " - Procesado palabra por palabra\n",
    "\n",
    " - Memoria a largo plazo usando un mecanismo llamado **atención**\n",
    " - procesamiento en paralelo.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso:\n",
    "\n",
    "<img src = './images/diagrama_transformers.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding Entrada: Convierte el texto en una serie de vectores, en tokens (representación numerica).\n",
    "\n",
    "\n",
    "- Codificacion de posicion: La secuencia se procesa en paralelo, es necesario indicar el orden en que se encuentran las palabras en el texto. Este codificador de posiciones genera vectores que se sumaran a los tokens y que indican la posicion relativa de cada token dentro de la secuencia.\n",
    "\n",
    "(pueden existir varios :))\n",
    "- **Bloque de codificacion**.\n",
    "  - - **atencional**: se encarga de analizar la totalidad de la secuencia y buscar relaciones a diferentes niveles dentro de la secuencia.\n",
    "Los tokens se llevan a 3 redes neuronales entrenadas para encontrar 3 vectores. **Queries**, **keys** y **values**. (3 representaciones alternativas de los tokes originales).\n",
    "\n",
    "Luego cada queries se compara con los keys, con esto se obtiene un puntaje para el grado de asociacion de pares de palabras \n",
    "(por ejemplo con similitud de coseno.). Con estos puntajes se pondera cada uno de los vectores **values**\n",
    "<center>\n",
    "<img src = 'images/atencional.png'>\n",
    "</center>\n",
    "\n",
    "Es necesario escalar los puntajes, escalandolos dividiendolo entre el tamaño de cada vector y llevandolo a una función softmax.\n",
    "\n",
    "<center>\n",
    "<img src = './images/diagrama_transformers_2.png'>\n",
    "</center>\n",
    "\n",
    "Luego se multiplica esta matrix con la matriz de values. Estos serán (tantos como palabras hay) nuevos tokes con la codificación de la información de contexto más relevante para cada palabra de la secuencia.\n",
    "\n",
    "**nota:** Un bloque atencional no es suficiente. Ya que por ejemplo \"I Love Euclidean Geometry\" puede tener varias asociaciones en un bloque atencional.  [I Love] [Euclidean Geometry] o [I] [Love] [Euclidean Geometry]. Es decir además de asociaciones entre palabras hay asociaciones entre frases.\n",
    "\n",
    "Si se usan multiples bloques atencionales, es posible encontrar asociaciones de palabras y frases en diferente niveles. Luego si se tienen $k$ bloques atencionales, estos se combinan con una red neuronal en un único vector por cada token\n",
    "\n",
    "<center>\n",
    "<img src = './images/diagrama_transformers_3.png'>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "  - -  **bloque residual:** En este bloque se lleva tanto la entrada como la salida del bloque atencional. Luego suma y normaliza estos datos para tener una escala apropiada para el siguiente bloque.\n",
    "\n",
    "  <center>\n",
    "<img src = './images/diagrama_transformers_4.png'>\n",
    "</center>\n",
    "\n",
    "\n",
    "  - - **red neuronal + bloque residual:**\n",
    "  La red neuronal procesa en paralelo todos los vectores de la secuencia. tomanda la info atencional de las capas anteriores y consolidandolas en una única representación. La entrada y salida de esta red neuronal son enviadas a un bloque residual que tiene las mismas caracteristicas del bloque anterior\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = 'images/decodificador_1.png'>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Embedding Salida** y **Codificación posicional**. (analogo a los anteriores anterior)\n",
    "\n",
    "- **Bloque Atencional con enmascaramiento** Codifica la relación entre diferentes elementos de salida, usando los **Queries** , **Keys** y **Values**, la gran diferencia es que cada palabra se genera de manera secuencial, el decodificador solo debe prestar atención debe prestar atención **unicamente** alas palabras generas anteriormente y no a las futuras\n",
    "\n",
    "<center>\n",
    "<img src = 'images/decodificador_2.png'>\n",
    "</center>\n",
    "\n",
    "- **Bloque atencional (decodificador)** A diferencia del bloque atencional anterior (codificación) este bloque centra su atención en la secuencia original como la de salida. Toma la salida del codificador a las redes **Queries** y **Keys**. Mientras que la red **values** tiene como entrada el dato proveniente del bloque residual anterior.\n",
    "\n",
    "Asi el codificar le comunica al decodificador a que elementos debe prestar mas atencios al momemnto de generar la secuencia de salida (analogamente se toman muchos bloques atencionales para codificar asociaciones). Luego de pasar por los decodificadores\n",
    "genera un vector con cantidades numericas.\n",
    "\n",
    "**Capa Lineal** Toma el resultado del decodificador y lo transforma en un vector más grande. Por ejemplo si el traductor aprende 10.000 palabras, entonces el vector de salida de la capa lineal tendra 10.000 elementos\n",
    "\n",
    "**Softmax** Esta capa convierte cada elemento de este vector en sus respectivas probabilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo traductor (ingles a español)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras_transformer import get_model, decode\n",
    "from pickle import load\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/english-spanish.pkl'\n",
    "df = load(open(filename , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:130,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( f'palabra en ingles: {df[1221,0]}')\n",
    "# print( f'palabra en español: {df[1221,1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palabra en ingles: be good\n",
      "palabra en ingles tokenizada: ['be', 'good']\n",
      "\n",
      "palabra en español: sean buenos\n",
      "palabra en español tokenizada: ['sean', 'buenos']\n"
     ]
    }
   ],
   "source": [
    "# Creacion de \"tokens\"\n",
    "# Transforma cada palabra en una lista de palabras, esto por cada palabra.\n",
    "\n",
    "# tokens en ingles\n",
    "source_tokens = []\n",
    "for sentencia in df[:, 0]:\n",
    "    source_tokens.append(sentencia.split(' '))\n",
    "\n",
    "# Tokens en español\n",
    "target_tokens = []\n",
    "for sentencia in df[:, 1]:\n",
    "    target_tokens.append(sentencia.split(' '))\n",
    "\n",
    "print(f'palabra en ingles: {df[120,0]}')\n",
    "print(f'palabra en ingles tokenizada: {source_tokens[120]}\\n')\n",
    "print(f'palabra en español: {df[120,1]}')\n",
    "print(f'palabra en español tokenizada: {target_tokens[120]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_dict(token_list):\n",
    "    \"\"\"Resumen\n",
    "\n",
    "    Args:\n",
    "        token_list: Una lista que contiene lista de frases tokenizadas.\n",
    "\n",
    "    Returns:\n",
    "        dict: retorna un diccionario, donde el 0 es padding, 1 es start 2 end, y el resto de palabras (todas diferentes) tiene como como key un único numero (el tamaño del diccionario antes de ser agregada).\n",
    "    \"\"\"\n",
    "    \n",
    "    token_dict = {\n",
    "        '<PAD>': 0,\n",
    "        '<START>': 1,\n",
    "        '<END>': 2\n",
    "    }\n",
    "    for tokens in token_list:\n",
    "        for token in tokens:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = len(token_dict)\n",
    "    return token_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<START>': 1,\n",
       " '<END>': 2,\n",
       " 've': 3,\n",
       " 'vete': 4,\n",
       " 'vaya': 5,\n",
       " 'vayase': 6,\n",
       " 'hola': 7,\n",
       " 'corre': 8,\n",
       " 'corran': 9,\n",
       " 'corra': 10,\n",
       " 'corred': 11,\n",
       " 'quien': 12,\n",
       " 'orale': 13,\n",
       " 'fuego': 14,\n",
       " 'incendio': 15,\n",
       " 'disparad': 16,\n",
       " 'ayuda': 17,\n",
       " 'socorro': 18,\n",
       " 'auxilio': 19,\n",
       " 'salta': 20,\n",
       " 'salte': 21,\n",
       " 'parad': 22,\n",
       " 'para': 23,\n",
       " 'pare': 24,\n",
       " 'espera': 25,\n",
       " 'esperen': 26,\n",
       " 'continua': 27,\n",
       " 'continue': 28,\n",
       " 'date': 29,\n",
       " 'prisa': 30,\n",
       " 'daos': 31,\n",
       " 'dese': 32,\n",
       " 'me': 33,\n",
       " 'oculte': 34,\n",
       " 'escondi': 35,\n",
       " 'ocultaba': 36,\n",
       " 'escondia': 37,\n",
       " 'corri': 38,\n",
       " 'corria': 39,\n",
       " 'lo': 40,\n",
       " 'intento': 41,\n",
       " 'he': 42,\n",
       " 'ganado': 43,\n",
       " 'oh': 44,\n",
       " 'no': 45,\n",
       " 'tomatelo': 46,\n",
       " 'con': 47,\n",
       " 'soda': 48,\n",
       " 'disparen': 49,\n",
       " 'dispara': 50,\n",
       " 'dispare': 51,\n",
       " 'sonrie': 52,\n",
       " 'al': 53,\n",
       " 'ataque': 54,\n",
       " 'atacad': 55,\n",
       " 'ataquen': 56,\n",
       " 'ataca': 57,\n",
       " 'levanta': 58,\n",
       " 'ahora': 59,\n",
       " 'mismo': 60,\n",
       " 'id': 61,\n",
       " 'vayan': 62,\n",
       " 'ya': 63,\n",
       " 'tengo': 64,\n",
       " 'pillas': 65,\n",
       " 'entendiste': 66,\n",
       " 'el': 67,\n",
       " 'corrio': 68,\n",
       " 'metete': 69,\n",
       " 'adentro': 70,\n",
       " 'abrazame': 71,\n",
       " 'preocupo': 72,\n",
       " 'cai': 73,\n",
       " 'hui': 74,\n",
       " 'escape': 75,\n",
       " 'huia': 76,\n",
       " 'escapaba': 77,\n",
       " 'yo': 78,\n",
       " 'se': 79,\n",
       " 'sali': 80,\n",
       " 'menti': 81,\n",
       " 'perdi': 82,\n",
       " 'dimito': 83,\n",
       " 'renuncie': 84,\n",
       " 'dejo': 85,\n",
       " 'cante': 86,\n",
       " 'llore': 87,\n",
       " 'lloraba': 88,\n",
       " 'estoy': 89,\n",
       " 'trabajando': 90,\n",
       " 'diecinueve': 91,\n",
       " 'levantado': 92,\n",
       " 'escucha': 93,\n",
       " 'escuche': 94,\n",
       " 'escuchen': 95,\n",
       " 'puede': 96,\n",
       " 'ser': 97,\n",
       " 'de': 98,\n",
       " 'ninguna': 99,\n",
       " 'manera': 100,\n",
       " 'imposible': 101,\n",
       " 'ningun': 102,\n",
       " 'modo': 103,\n",
       " 'eso': 104,\n",
       " 'nada': 105,\n",
       " 'ni': 106,\n",
       " 'cagando': 107,\n",
       " 'mangos': 108,\n",
       " 'minga': 109,\n",
       " 'en': 110,\n",
       " 'pedo': 111,\n",
       " 'serio': 112,\n",
       " 'la': 113,\n",
       " 'verdad': 114,\n",
       " 'gracias': 115,\n",
       " 'pruebalo': 116,\n",
       " 'procuramos': 117,\n",
       " 'ganamos': 118,\n",
       " 'por': 119,\n",
       " 'que': 120,\n",
       " 'preguntale': 121,\n",
       " 'a': 122,\n",
       " 'tom': 123,\n",
       " 'mantente': 124,\n",
       " 'calma': 125,\n",
       " 'estate': 126,\n",
       " 'tranquilo': 127,\n",
       " 'justo': 128,\n",
       " 'bueno': 129,\n",
       " 'buena': 130,\n",
       " 'sed': 131,\n",
       " 'buenas': 132,\n",
       " 'buenos': 133,\n",
       " 'sea': 134,\n",
       " 'sean': 135,\n",
       " 'gentiles': 136,\n",
       " 'agradable': 137,\n",
       " 'pirate': 138,\n",
       " 'llamame': 139,\n",
       " 'llamadme': 140,\n",
       " 'llamanos': 141,\n",
       " 'entre': 142,\n",
       " 'pase': 143}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokens_dict = build_token_dict(source_tokens)\n",
    "target_tokens_dict = build_token_dict(target_tokens)\n",
    "target_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diccionario con llave el label de la palabra y key la palabra\n",
    "target_tokens_dict_inv = {palabra: label for label,\n",
    "                          palabra in target_tokens_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<START>': 1,\n",
       " '<END>': 2,\n",
       " 'go': 3,\n",
       " 'hi': 4,\n",
       " 'run': 5,\n",
       " 'who': 6,\n",
       " 'wow': 7,\n",
       " 'fire': 8,\n",
       " 'help': 9,\n",
       " 'jump': 10,\n",
       " 'stop': 11,\n",
       " 'wait': 12,\n",
       " 'on': 13,\n",
       " 'hello': 14,\n",
       " 'hurry': 15,\n",
       " 'i': 16,\n",
       " 'hid': 17,\n",
       " 'ran': 18,\n",
       " 'try': 19,\n",
       " 'won': 20,\n",
       " 'oh': 21,\n",
       " 'no': 22,\n",
       " 'relax': 23,\n",
       " 'shoot': 24,\n",
       " 'smile': 25,\n",
       " 'attack': 26,\n",
       " 'get': 27,\n",
       " 'up': 28,\n",
       " 'now': 29,\n",
       " 'got': 30,\n",
       " 'it': 31,\n",
       " 'he': 32,\n",
       " 'hop': 33,\n",
       " 'in': 34,\n",
       " 'hug': 35,\n",
       " 'me': 36,\n",
       " 'care': 37,\n",
       " 'fell': 38,\n",
       " 'fled': 39,\n",
       " 'know': 40,\n",
       " 'left': 41,\n",
       " 'lied': 42,\n",
       " 'lost': 43,\n",
       " 'quit': 44,\n",
       " 'sang': 45,\n",
       " 'wept': 46,\n",
       " 'work': 47,\n",
       " 'im': 48,\n",
       " 'listen': 49,\n",
       " 'way': 50,\n",
       " 'really': 51,\n",
       " 'thanks': 52,\n",
       " 'we': 53,\n",
       " 'why': 54,\n",
       " 'ask': 55,\n",
       " 'tom': 56,\n",
       " 'awesome': 57,\n",
       " 'be': 58,\n",
       " 'calm': 59,\n",
       " 'cool': 60,\n",
       " 'fair': 61,\n",
       " 'good': 62,\n",
       " 'kind': 63,\n",
       " 'nice': 64,\n",
       " 'beat': 65,\n",
       " 'call': 66,\n",
       " 'us': 67,\n",
       " 'come': 68}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar start, end y pad a cada frase del entrenamiento\n",
    "\n",
    "encoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "decoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "\n",
    "# salida del transformer.\n",
    "output_tokens = [tokens + ['<END>'] for tokens in target_tokens]\n",
    "\n",
    "# Tamaño maximo de una palabra de entrada y de salida\n",
    "source_max_len = max(map(len, encoder_tokens))\n",
    "target_max_len = max(map(len, decoder_tokens))\n",
    "\n",
    "## Tenemos que entregar una lista con la misma dimensión. Dado que hay textos con diferentes tamaños \n",
    "encoder_tokens = [tokens + ['<PAD>'] *\n",
    "                  (source_max_len - len(tokens)) for tokens in encoder_tokens]\n",
    "\n",
    "decoder_tokens = [tokens + ['<PAD>'] *\n",
    "                  (target_max_len - len(tokens)) for tokens in decoder_tokens]\n",
    "\n",
    "\n",
    "## La salida final será en español por lo que se llenan espacios con\n",
    "output_tokens = [tokens + ['<PAD>'] *\n",
    "                  (source_max_len - len(tokens)) for tokens in output_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación posicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformar cada palabra en su respectiva etiqueta.\n",
    "encoder_input = [list(map(lambda x: source_tokens_dict[x], tokens))\n",
    "                 for tokens in encoder_tokens]\n",
    "\n",
    "decoder_input = [list(map(lambda x: target_tokens_dict[x], tokens))\n",
    "                 for tokens in decoder_tokens]\n",
    "\n",
    "## TODO revisar mas adelante minuto 29:41\n",
    "\n",
    "output_decoded = [list(map(lambda x: [target_tokens_dict[x]], tokens))  \n",
    "                  for tokens in output_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **token_num** : Tamaño máximo que puede llegar a tener cada frase.\n",
    "- **embed_dim** : Dimensión del embedding de entrada\n",
    "- **encoder_num**: Número de codificadores que tiene la entrada.\n",
    "- **decoder_num**: Número de decodificadores que tiene la salida\n",
    "- **head_num**: Número de bloques atencionales (para encontrar relaciones en diferentes niveles)\n",
    "- **hidden_dim**: Número de neuronas en la capa oculta (La capa verde en la teoria)\n",
    "- **dropout_rate**: Porcentajes de neuronas desactivadas aleatoriamente durante el proceso\n",
    "- **use_same_embed** (Boolean) donde \"False\" indica que la forma de representar la frase en ingles va a ser diferente a la forma en que va a encontrar las frases en español.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<START>': 1,\n",
       " '<END>': 2,\n",
       " 'go': 3,\n",
       " 'hi': 4,\n",
       " 'run': 5,\n",
       " 'who': 6,\n",
       " 'wow': 7,\n",
       " 'fire': 8,\n",
       " 'help': 9,\n",
       " 'jump': 10,\n",
       " 'stop': 11,\n",
       " 'wait': 12,\n",
       " 'on': 13,\n",
       " 'hello': 14,\n",
       " 'hurry': 15,\n",
       " 'i': 16,\n",
       " 'hid': 17,\n",
       " 'ran': 18,\n",
       " 'try': 19,\n",
       " 'won': 20,\n",
       " 'oh': 21,\n",
       " 'no': 22,\n",
       " 'relax': 23,\n",
       " 'shoot': 24,\n",
       " 'smile': 25,\n",
       " 'attack': 26,\n",
       " 'get': 27,\n",
       " 'up': 28,\n",
       " 'now': 29,\n",
       " 'got': 30,\n",
       " 'it': 31,\n",
       " 'he': 32,\n",
       " 'hop': 33,\n",
       " 'in': 34,\n",
       " 'hug': 35,\n",
       " 'me': 36,\n",
       " 'care': 37,\n",
       " 'fell': 38,\n",
       " 'fled': 39,\n",
       " 'know': 40,\n",
       " 'left': 41,\n",
       " 'lied': 42,\n",
       " 'lost': 43,\n",
       " 'quit': 44,\n",
       " 'sang': 45,\n",
       " 'wept': 46,\n",
       " 'work': 47,\n",
       " 'im': 48,\n",
       " 'listen': 49,\n",
       " 'way': 50,\n",
       " 'really': 51,\n",
       " 'thanks': 52,\n",
       " 'we': 53,\n",
       " 'why': 54,\n",
       " 'ask': 55,\n",
       " 'tom': 56,\n",
       " 'awesome': 57,\n",
       " 'be': 58,\n",
       " 'calm': 59,\n",
       " 'cool': 60,\n",
       " 'fair': 61,\n",
       " 'good': 62,\n",
       " 'kind': 63,\n",
       " 'nice': 64,\n",
       " 'beat': 65,\n",
       " 'call': 66,\n",
       " 'us': 67,\n",
       " 'come': 68}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to EmbeddingRet: {'weights': None}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb Cell 29\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Embedding\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m encoder_embed_layer \u001b[39m=\u001b[39m Embedding(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     input_dim\u001b[39m=\u001b[39m\u001b[39m144\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     output_dim\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, \u001b[39m# Assuming you had dropout_rate defined\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m get_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     token_num\u001b[39m=\u001b[39;49m\u001b[39mmax\u001b[39;49m(\u001b[39mlen\u001b[39;49m(source_tokens_dict), \u001b[39mlen\u001b[39;49m(target_tokens_dict)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     embed_dim\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     encoder_num\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     decoder_num\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     head_num\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     hidden_dim\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     dropout_rate\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     use_same_embed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# model.compile('adam','sparse_categorical_crossentropy')\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/quipux/induccion/plan_formacion/env/lib/python3.10/site-packages/keras_transformer/transformer.py:360\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(token_num, embed_dim, encoder_num, decoder_num, head_num, hidden_dim, attention_activation, feed_forward_activation, dropout_rate, use_same_embed, embed_weights, embed_trainable, trainable)\u001b[0m\n\u001b[1;32m    351\u001b[0m     encoder_embed_layer \u001b[39m=\u001b[39m decoder_embed_layer \u001b[39m=\u001b[39m EmbeddingRet(\n\u001b[1;32m    352\u001b[0m         input_dim\u001b[39m=\u001b[39mencoder_token_num,\n\u001b[1;32m    353\u001b[0m         output_dim\u001b[39m=\u001b[39membed_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mToken-Embedding\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    359\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     encoder_embed_layer \u001b[39m=\u001b[39m EmbeddingRet(\n\u001b[1;32m    361\u001b[0m         input_dim\u001b[39m=\u001b[39;49mencoder_token_num,\n\u001b[1;32m    362\u001b[0m         output_dim\u001b[39m=\u001b[39;49membed_dim,\n\u001b[1;32m    363\u001b[0m         mask_zero\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    364\u001b[0m         weights\u001b[39m=\u001b[39;49mencoder_embed_weights,\n\u001b[1;32m    365\u001b[0m         trainable\u001b[39m=\u001b[39;49mencoder_embed_trainable,\n\u001b[1;32m    366\u001b[0m         name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mEncoder-Token-Embedding\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    367\u001b[0m     )\n\u001b[1;32m    368\u001b[0m     decoder_embed_layer \u001b[39m=\u001b[39m EmbeddingRet(\n\u001b[1;32m    369\u001b[0m         input_dim\u001b[39m=\u001b[39mdecoder_token_num,\n\u001b[1;32m    370\u001b[0m         output_dim\u001b[39m=\u001b[39membed_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDecoder-Token-Embedding\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    375\u001b[0m     )\n\u001b[1;32m    376\u001b[0m encoder_input \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m,), name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEncoder-Input\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/quipux/induccion/plan_formacion/env/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dim \u001b[39m=\u001b[39m input_dim\n\u001b[1;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim \u001b[39m=\u001b[39m output_dim\n",
      "File \u001b[0;32m~/Documentos/quipux/induccion/plan_formacion/env/lib/python3.10/site-packages/keras/src/layers/layer.py:264\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_shape_arg \u001b[39m=\u001b[39m input_shape_arg\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnrecognized keyword arguments \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpassed to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast \u001b[39m=\u001b[39m autocast\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to EmbeddingRet: {'weights': None}"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "model = get_model(\n",
    "    token_num=max(len(source_tokens_dict), len(target_tokens_dict)),\n",
    "    embed_dim=32,\n",
    "    encoder_num=3,\n",
    "    decoder_num=2,\n",
    "    head_num=4,\n",
    "    hidden_dim=128,\n",
    "    dropout_rate=0.05,\n",
    "    use_same_embed=False,\n",
    ")\n",
    "\n",
    "# model.compile('adam','sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb Cell 31\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# No tengo suficiente memoria :( \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# model.fit(x,y, epochs=15, batch_size= 32)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m filename_model \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodels/translator.h5\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39mload_weights(filename_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "## Entrenamiento\n",
    "x = [np.array(encoder_input), np.array(decoder_input)]\n",
    "y = [np.array(output_decoded)]\n",
    "\n",
    "# No tengo suficiente memoria :( \n",
    "# model.fit(x,y, epochs=15, batch_size= 32)\n",
    "\n",
    "filename_model = 'models/translator.h5'\n",
    "model.load_weights(filename_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentencia):\n",
    "    sentencia_tokens = [tokens + ['<END>', '<PAD>']\n",
    "                        for tokens in [sentencia.split(' ')]]\n",
    "    tr_input = [list(map(lambda x: source_tokens_dict[x], tokens))\n",
    "                for tokens in sentencia_tokens]\n",
    "\n",
    "    # decodificacion\n",
    "    decoded = decode(\n",
    "        model,\n",
    "        tr_input,\n",
    "        start_token=target_tokens_dict['<START>'],\n",
    "        end_token=target_tokens_dict['<END>'],\n",
    "        pad_token=target_tokens_dict['<PAD>']\n",
    "    )\n",
    "    print(f\"Frase original {sentencia}\")\n",
    "    \n",
    "    print(f\"Frase traducida {' '.join(map(lambda x :target_tokens_dict_inv[x], decoded[1:-1]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tr_input**: Es la entrada del transformer, agregando < END>  y < PAD> (START no es necesario)\n",
    "\n",
    "- **decoded[1:-1]** me va a retornar los label de cada frase traducida, luego obtenemos la palabra correspondiente en target_tokens_dict_inv. \n",
    "\n",
    "- **start_token , end_token, pad_token**: deben ser los label que tomamos para indicar donde empieza una frase,donde termina y donde dejamos espacios respectivamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"The red car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to EmbeddingRet: {'weights': [array([[6.00212868e-01, 9.63197295e-01, 1.47801334e-01, 2.56916644e-01,\n        8.73556827e-01, 4.91892232e-01, 8.98961092e-01, 1.85517898e-01,\n        5.32668587e-01, 3.26269633e-01, 3.16542560e-01, 4.46876964e-01,\n        4.33077449e-01, 3.57346880e-01, 9.14970770e-01, 7.31744185e-01,\n        7.27546991e-01, 2.89913450e-01, 5.77709424e-01, 7.79179433e-01,\n        7.95590369e-01, 3.44530461e-01, 7.70872757e-01, 7.35893897e-01,\n        1.41506486e-01, 8.65945469e-01, 4.41321470e-01, 4.86410449e-01,\n        4.48369179e-01, 5.67846001e-01],\n       [6.21169247e-01, 4.98179566e-01, 8.66788543e-01, 6.27734756e-01,\n        4.01427949e-01, 4.16691757e-01, 8.10838615e-01, 3.48191943e-01,\n        2.11454796e-01, 5.93831880e-02, 8.76026848e-01, 9.18546451e-01,\n        1.20120182e-01, 3.34473741e-01, 1.75372070e-01, 1.15898469e-01,\n        8.99866743e-01, 5.68772591e-02, 9.80485663e-01, 9.64508607e-02,\n        8.63470649e-01, 5.66506107e-01, 3.67917488e-01, 3.42342377e-01,\n        7.57364143e-01, 3.14573295e-01, 6.57318917e-01, 5.17326084e-01,\n        4.84965645e-01, 9.01162171e-01],\n       [5.54645059e-01, 8.26861603e-01, 7.25573534e-01, 3.85572461e-02,\n        7.73110053e-01, 2.16870250e-01, 9.03149647e-01, 4.29241906e-02,\n        3.33072034e-01, 9.97329472e-02, 4.75589117e-01, 8.20022436e-01,\n        2.98187360e-01, 1.50934897e-01, 3.30267036e-01, 8.13880142e-01,\n        1.40383958e-01, 2.27362449e-01, 6.88519645e-02, 7.05710044e-01,\n        3.95233244e-01, 3.10839977e-01, 7.18626390e-01, 3.35977542e-01,\n        7.27771273e-01, 8.15199395e-01, 2.17662843e-01, 9.73818697e-01,\n        1.62357948e-01, 2.90840907e-01],\n       [1.79795291e-01, 3.45505656e-01, 4.80060888e-01, 5.22175869e-01,\n        8.53606042e-01, 8.89447909e-01, 2.20103861e-01, 6.22894032e-01,\n        1.11496057e-01, 4.58969860e-01, 3.22333538e-01, 3.16500745e-01,\n        4.82584242e-01, 7.29827636e-01, 6.91826588e-02, 8.79173338e-01,\n        7.34813775e-01, 1.76499389e-01, 9.39160909e-01, 5.06312224e-01,\n        9.99808578e-01, 1.97259474e-01, 5.34908198e-01, 2.90248043e-01,\n        3.04173557e-01, 5.91065381e-01, 9.21719067e-01, 8.05263856e-01,\n        7.23941399e-01, 5.59173782e-01],\n       [9.22298504e-01, 4.92361407e-01, 8.73832178e-01, 8.33981644e-01,\n        2.13835347e-01, 7.71225463e-01, 1.21711569e-02, 3.22829538e-01,\n        2.29567445e-01, 5.06862958e-01, 7.36853162e-01, 9.76763674e-02,\n        5.14922202e-01, 9.38412022e-01, 2.28646551e-01, 6.77141144e-01,\n        5.92880271e-01, 1.00636957e-02, 4.75826196e-01, 7.08770391e-01,\n        4.39754320e-02, 8.79521483e-01, 5.20081417e-01, 3.06610483e-02,\n        2.24413612e-01, 9.53675696e-01, 5.82319733e-01, 1.07472568e-01,\n        2.87544502e-01, 4.56703626e-01],\n       [2.09500693e-02, 4.11615514e-01, 4.89458635e-01, 2.43677875e-01,\n        5.88639000e-01, 7.53240120e-01, 2.35834224e-01, 6.20499900e-01,\n        6.39622243e-01, 9.48540301e-01, 7.78276167e-01, 8.48345270e-01,\n        4.90419908e-01, 1.85348587e-01, 9.95815293e-01, 1.29355761e-01,\n        4.71457319e-01, 6.80930992e-02, 9.43850857e-01, 9.64924941e-01,\n        7.19389062e-01, 3.49992844e-01, 2.54382401e-01, 2.65303325e-01,\n        1.27294025e-01, 5.25808953e-01, 1.41817276e-01, 3.16730667e-01,\n        6.26706476e-01, 7.27543610e-01],\n       [2.42727046e-02, 4.30115984e-01, 6.52124595e-01, 8.53245976e-01,\n        4.75324782e-01, 9.69205872e-01, 2.65632548e-01, 1.35087066e-02,\n        4.83752865e-01, 2.56113795e-01, 8.23717672e-01, 2.32772672e-01,\n        3.10629218e-01, 7.91227431e-01, 7.15143252e-01, 5.58051237e-01,\n        7.04948062e-01, 4.18636864e-01, 5.31004761e-03, 1.13551285e-02,\n        5.11221788e-01, 8.32909797e-02, 5.10754802e-02, 9.65516639e-01,\n        8.59002640e-01, 1.52027227e-01, 6.64218590e-04, 9.41667795e-01,\n        2.78325298e-01, 1.85897603e-01],\n       [6.91508108e-01, 1.08903739e-01, 2.64649598e-01, 9.75094680e-01,\n        6.39462774e-01, 5.20677791e-01, 3.97918615e-01, 7.74500955e-01,\n        1.40957477e-01, 9.67337802e-01, 8.61123008e-01, 6.17656983e-01,\n        4.29061904e-02, 7.00855649e-01, 9.13284341e-01, 5.24577067e-01,\n        3.54224822e-01, 1.20277345e-01, 7.54901104e-01, 8.85021851e-01,\n        1.00251744e-01, 7.58984555e-01, 1.70604863e-02, 9.67054918e-01,\n        6.15058021e-01, 5.52439059e-01, 2.95949834e-01, 9.29291672e-01,\n        2.65905627e-01, 8.28146613e-01],\n       [9.85108679e-01, 7.83396646e-01, 5.18989920e-01, 6.60742639e-02,\n        4.72413789e-01, 4.38255947e-01, 2.02796041e-01, 4.23587637e-01,\n        3.57757884e-01, 1.63684261e-01, 4.41374143e-01, 2.62799956e-01,\n        5.22062421e-01, 3.51600597e-02, 9.06231420e-01, 8.16364306e-01,\n        5.52581333e-01, 8.51808583e-01, 9.62395074e-01, 1.10522294e-01,\n        6.30831808e-01, 9.97994001e-01, 9.87889169e-01, 6.03322992e-01,\n        1.28020870e-01, 5.83192831e-01, 2.06463557e-03, 1.98911335e-01,\n        9.56123160e-01, 3.30440573e-01],\n       [6.38390106e-01, 2.80859495e-01, 9.47821887e-01, 7.28558730e-01,\n        3.29651158e-01, 7.91761421e-01, 1.08165524e-01, 3.92318940e-01,\n        2.21218128e-01, 6.83726447e-01, 1.02446282e-01, 3.97025832e-01,\n        2.76649730e-01, 5.06342919e-01, 3.49897681e-01, 7.06410578e-01,\n        2.45770243e-02, 6.33986921e-01, 2.30571290e-01, 2.68709029e-01,\n        8.00255604e-01, 9.55568394e-01, 3.16550210e-01, 8.26805270e-01,\n        1.03990838e-01, 6.33981653e-01, 7.51032300e-01, 1.55977928e-01,\n        4.26002388e-01, 8.92707164e-01],\n       [1.03578463e-01, 1.80963582e-02, 5.90585379e-01, 4.35531541e-01,\n        7.98689249e-01, 9.23455538e-01, 2.99153645e-01, 3.88404117e-01,\n        4.86272086e-01, 5.88151460e-01, 9.83853830e-01, 6.97330251e-01,\n        3.89548507e-01, 2.63767686e-01, 9.44625718e-01, 1.35548433e-01,\n        7.20265853e-01, 9.25395025e-01, 6.64665587e-01, 4.23054440e-01,\n        1.98990940e-01, 3.67475322e-01, 7.06871809e-01, 6.49534224e-01,\n        9.27976167e-01, 8.66860914e-01, 8.16150752e-01, 9.11450875e-01,\n        2.76337153e-01, 3.69523540e-01],\n       [3.79893904e-01, 5.60450589e-01, 6.68218230e-01, 2.86716683e-01,\n        1.94624673e-02, 3.99222384e-01, 3.08527960e-01, 9.42184719e-01,\n        8.88265041e-01, 8.60310678e-01, 6.52999761e-01, 3.44289165e-01,\n        5.48849267e-01, 8.15225041e-01, 9.86103687e-02, 8.01074880e-01,\n        4.11797913e-02, 8.16421031e-01, 8.07563804e-01, 5.10073088e-02,\n        6.27160711e-01, 5.02453074e-01, 1.69819503e-01, 1.48378938e-01,\n        7.73259126e-01, 5.67692749e-01, 9.82999135e-01, 9.82247777e-01,\n        9.92666993e-01, 1.18615518e-01],\n       [9.38256137e-01, 2.44569609e-01, 4.58212260e-01, 7.57406556e-01,\n        2.03620932e-01, 5.66311606e-01, 1.85816748e-01, 1.04736107e-01,\n        1.16558612e-01, 3.57639035e-01, 4.65483684e-03, 4.24853921e-01,\n        6.64197105e-01, 4.01688185e-01, 8.57946005e-02, 6.26888620e-02,\n        2.78116513e-01, 1.69312691e-01, 9.65094973e-01, 1.51230225e-01,\n        8.05462437e-01, 5.86107942e-01, 5.69286920e-01, 5.12080716e-01,\n        9.71763076e-01, 3.63844775e-01, 7.87915751e-01, 5.55294107e-01,\n        3.95633668e-01, 9.55465933e-01]])]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m get_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     token_num\u001b[39m=\u001b[39;49m\u001b[39m144\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     embed_dim\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     encoder_num\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     decoder_num\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     head_num\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     hidden_dim\u001b[39m=\u001b[39;49m\u001b[39m120\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     attention_activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     feed_forward_activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     dropout_rate\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     embed_weights\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandom((\u001b[39m13\u001b[39;49m, \u001b[39m30\u001b[39;49m)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pc/Documentos/quipux/induccion/plan_formacion/transformer/main.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documentos/quipux/induccion/plan_formacion/env/lib/python3.10/site-packages/keras_transformer/transformer.py:351\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(token_num, embed_dim, encoder_num, decoder_num, head_num, hidden_dim, attention_activation, feed_forward_activation, dropout_rate, use_same_embed, embed_weights, embed_trainable, trainable)\u001b[0m\n\u001b[1;32m    348\u001b[0m     decoder_embed_trainable \u001b[39m=\u001b[39m decoder_embed_weights \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m use_same_embed:\n\u001b[0;32m--> 351\u001b[0m     encoder_embed_layer \u001b[39m=\u001b[39m decoder_embed_layer \u001b[39m=\u001b[39m EmbeddingRet(\n\u001b[1;32m    352\u001b[0m         input_dim\u001b[39m=\u001b[39;49mencoder_token_num,\n\u001b[1;32m    353\u001b[0m         output_dim\u001b[39m=\u001b[39;49membed_dim,\n\u001b[1;32m    354\u001b[0m         mask_zero\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    355\u001b[0m         weights\u001b[39m=\u001b[39;49mencoder_embed_weights,\n\u001b[1;32m    356\u001b[0m         trainable\u001b[39m=\u001b[39;49mencoder_embed_trainable,\n\u001b[1;32m    357\u001b[0m         name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mToken-Embedding\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    359\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     encoder_embed_layer \u001b[39m=\u001b[39m EmbeddingRet(\n\u001b[1;32m    361\u001b[0m         input_dim\u001b[39m=\u001b[39mencoder_token_num,\n\u001b[1;32m    362\u001b[0m         output_dim\u001b[39m=\u001b[39membed_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEncoder-Token-Embedding\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    367\u001b[0m     )\n",
      "File \u001b[0;32m~/Documentos/quipux/induccion/plan_formacion/env/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dim \u001b[39m=\u001b[39m input_dim\n\u001b[1;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim \u001b[39m=\u001b[39m output_dim\n",
      "File \u001b[0;32m~/Documentos/quipux/induccion/plan_formacion/env/lib/python3.10/site-packages/keras/src/layers/layer.py:264\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_shape_arg \u001b[39m=\u001b[39m input_shape_arg\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnrecognized keyword arguments \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpassed to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast \u001b[39m=\u001b[39m autocast\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to EmbeddingRet: {'weights': [array([[6.00212868e-01, 9.63197295e-01, 1.47801334e-01, 2.56916644e-01,\n        8.73556827e-01, 4.91892232e-01, 8.98961092e-01, 1.85517898e-01,\n        5.32668587e-01, 3.26269633e-01, 3.16542560e-01, 4.46876964e-01,\n        4.33077449e-01, 3.57346880e-01, 9.14970770e-01, 7.31744185e-01,\n        7.27546991e-01, 2.89913450e-01, 5.77709424e-01, 7.79179433e-01,\n        7.95590369e-01, 3.44530461e-01, 7.70872757e-01, 7.35893897e-01,\n        1.41506486e-01, 8.65945469e-01, 4.41321470e-01, 4.86410449e-01,\n        4.48369179e-01, 5.67846001e-01],\n       [6.21169247e-01, 4.98179566e-01, 8.66788543e-01, 6.27734756e-01,\n        4.01427949e-01, 4.16691757e-01, 8.10838615e-01, 3.48191943e-01,\n        2.11454796e-01, 5.93831880e-02, 8.76026848e-01, 9.18546451e-01,\n        1.20120182e-01, 3.34473741e-01, 1.75372070e-01, 1.15898469e-01,\n        8.99866743e-01, 5.68772591e-02, 9.80485663e-01, 9.64508607e-02,\n        8.63470649e-01, 5.66506107e-01, 3.67917488e-01, 3.42342377e-01,\n        7.57364143e-01, 3.14573295e-01, 6.57318917e-01, 5.17326084e-01,\n        4.84965645e-01, 9.01162171e-01],\n       [5.54645059e-01, 8.26861603e-01, 7.25573534e-01, 3.85572461e-02,\n        7.73110053e-01, 2.16870250e-01, 9.03149647e-01, 4.29241906e-02,\n        3.33072034e-01, 9.97329472e-02, 4.75589117e-01, 8.20022436e-01,\n        2.98187360e-01, 1.50934897e-01, 3.30267036e-01, 8.13880142e-01,\n        1.40383958e-01, 2.27362449e-01, 6.88519645e-02, 7.05710044e-01,\n        3.95233244e-01, 3.10839977e-01, 7.18626390e-01, 3.35977542e-01,\n        7.27771273e-01, 8.15199395e-01, 2.17662843e-01, 9.73818697e-01,\n        1.62357948e-01, 2.90840907e-01],\n       [1.79795291e-01, 3.45505656e-01, 4.80060888e-01, 5.22175869e-01,\n        8.53606042e-01, 8.89447909e-01, 2.20103861e-01, 6.22894032e-01,\n        1.11496057e-01, 4.58969860e-01, 3.22333538e-01, 3.16500745e-01,\n        4.82584242e-01, 7.29827636e-01, 6.91826588e-02, 8.79173338e-01,\n        7.34813775e-01, 1.76499389e-01, 9.39160909e-01, 5.06312224e-01,\n        9.99808578e-01, 1.97259474e-01, 5.34908198e-01, 2.90248043e-01,\n        3.04173557e-01, 5.91065381e-01, 9.21719067e-01, 8.05263856e-01,\n        7.23941399e-01, 5.59173782e-01],\n       [9.22298504e-01, 4.92361407e-01, 8.73832178e-01, 8.33981644e-01,\n        2.13835347e-01, 7.71225463e-01, 1.21711569e-02, 3.22829538e-01,\n        2.29567445e-01, 5.06862958e-01, 7.36853162e-01, 9.76763674e-02,\n        5.14922202e-01, 9.38412022e-01, 2.28646551e-01, 6.77141144e-01,\n        5.92880271e-01, 1.00636957e-02, 4.75826196e-01, 7.08770391e-01,\n        4.39754320e-02, 8.79521483e-01, 5.20081417e-01, 3.06610483e-02,\n        2.24413612e-01, 9.53675696e-01, 5.82319733e-01, 1.07472568e-01,\n        2.87544502e-01, 4.56703626e-01],\n       [2.09500693e-02, 4.11615514e-01, 4.89458635e-01, 2.43677875e-01,\n        5.88639000e-01, 7.53240120e-01, 2.35834224e-01, 6.20499900e-01,\n        6.39622243e-01, 9.48540301e-01, 7.78276167e-01, 8.48345270e-01,\n        4.90419908e-01, 1.85348587e-01, 9.95815293e-01, 1.29355761e-01,\n        4.71457319e-01, 6.80930992e-02, 9.43850857e-01, 9.64924941e-01,\n        7.19389062e-01, 3.49992844e-01, 2.54382401e-01, 2.65303325e-01,\n        1.27294025e-01, 5.25808953e-01, 1.41817276e-01, 3.16730667e-01,\n        6.26706476e-01, 7.27543610e-01],\n       [2.42727046e-02, 4.30115984e-01, 6.52124595e-01, 8.53245976e-01,\n        4.75324782e-01, 9.69205872e-01, 2.65632548e-01, 1.35087066e-02,\n        4.83752865e-01, 2.56113795e-01, 8.23717672e-01, 2.32772672e-01,\n        3.10629218e-01, 7.91227431e-01, 7.15143252e-01, 5.58051237e-01,\n        7.04948062e-01, 4.18636864e-01, 5.31004761e-03, 1.13551285e-02,\n        5.11221788e-01, 8.32909797e-02, 5.10754802e-02, 9.65516639e-01,\n        8.59002640e-01, 1.52027227e-01, 6.64218590e-04, 9.41667795e-01,\n        2.78325298e-01, 1.85897603e-01],\n       [6.91508108e-01, 1.08903739e-01, 2.64649598e-01, 9.75094680e-01,\n        6.39462774e-01, 5.20677791e-01, 3.97918615e-01, 7.74500955e-01,\n        1.40957477e-01, 9.67337802e-01, 8.61123008e-01, 6.17656983e-01,\n        4.29061904e-02, 7.00855649e-01, 9.13284341e-01, 5.24577067e-01,\n        3.54224822e-01, 1.20277345e-01, 7.54901104e-01, 8.85021851e-01,\n        1.00251744e-01, 7.58984555e-01, 1.70604863e-02, 9.67054918e-01,\n        6.15058021e-01, 5.52439059e-01, 2.95949834e-01, 9.29291672e-01,\n        2.65905627e-01, 8.28146613e-01],\n       [9.85108679e-01, 7.83396646e-01, 5.18989920e-01, 6.60742639e-02,\n        4.72413789e-01, 4.38255947e-01, 2.02796041e-01, 4.23587637e-01,\n        3.57757884e-01, 1.63684261e-01, 4.41374143e-01, 2.62799956e-01,\n        5.22062421e-01, 3.51600597e-02, 9.06231420e-01, 8.16364306e-01,\n        5.52581333e-01, 8.51808583e-01, 9.62395074e-01, 1.10522294e-01,\n        6.30831808e-01, 9.97994001e-01, 9.87889169e-01, 6.03322992e-01,\n        1.28020870e-01, 5.83192831e-01, 2.06463557e-03, 1.98911335e-01,\n        9.56123160e-01, 3.30440573e-01],\n       [6.38390106e-01, 2.80859495e-01, 9.47821887e-01, 7.28558730e-01,\n        3.29651158e-01, 7.91761421e-01, 1.08165524e-01, 3.92318940e-01,\n        2.21218128e-01, 6.83726447e-01, 1.02446282e-01, 3.97025832e-01,\n        2.76649730e-01, 5.06342919e-01, 3.49897681e-01, 7.06410578e-01,\n        2.45770243e-02, 6.33986921e-01, 2.30571290e-01, 2.68709029e-01,\n        8.00255604e-01, 9.55568394e-01, 3.16550210e-01, 8.26805270e-01,\n        1.03990838e-01, 6.33981653e-01, 7.51032300e-01, 1.55977928e-01,\n        4.26002388e-01, 8.92707164e-01],\n       [1.03578463e-01, 1.80963582e-02, 5.90585379e-01, 4.35531541e-01,\n        7.98689249e-01, 9.23455538e-01, 2.99153645e-01, 3.88404117e-01,\n        4.86272086e-01, 5.88151460e-01, 9.83853830e-01, 6.97330251e-01,\n        3.89548507e-01, 2.63767686e-01, 9.44625718e-01, 1.35548433e-01,\n        7.20265853e-01, 9.25395025e-01, 6.64665587e-01, 4.23054440e-01,\n        1.98990940e-01, 3.67475322e-01, 7.06871809e-01, 6.49534224e-01,\n        9.27976167e-01, 8.66860914e-01, 8.16150752e-01, 9.11450875e-01,\n        2.76337153e-01, 3.69523540e-01],\n       [3.79893904e-01, 5.60450589e-01, 6.68218230e-01, 2.86716683e-01,\n        1.94624673e-02, 3.99222384e-01, 3.08527960e-01, 9.42184719e-01,\n        8.88265041e-01, 8.60310678e-01, 6.52999761e-01, 3.44289165e-01,\n        5.48849267e-01, 8.15225041e-01, 9.86103687e-02, 8.01074880e-01,\n        4.11797913e-02, 8.16421031e-01, 8.07563804e-01, 5.10073088e-02,\n        6.27160711e-01, 5.02453074e-01, 1.69819503e-01, 1.48378938e-01,\n        7.73259126e-01, 5.67692749e-01, 9.82999135e-01, 9.82247777e-01,\n        9.92666993e-01, 1.18615518e-01],\n       [9.38256137e-01, 2.44569609e-01, 4.58212260e-01, 7.57406556e-01,\n        2.03620932e-01, 5.66311606e-01, 1.85816748e-01, 1.04736107e-01,\n        1.16558612e-01, 3.57639035e-01, 4.65483684e-03, 4.24853921e-01,\n        6.64197105e-01, 4.01688185e-01, 8.57946005e-02, 6.26888620e-02,\n        2.78116513e-01, 1.69312691e-01, 9.65094973e-01, 1.51230225e-01,\n        8.05462437e-01, 5.86107942e-01, 5.69286920e-01, 5.12080716e-01,\n        9.71763076e-01, 3.63844775e-01, 7.87915751e-01, 5.55294107e-01,\n        3.95633668e-01, 9.55465933e-01]])]}"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
